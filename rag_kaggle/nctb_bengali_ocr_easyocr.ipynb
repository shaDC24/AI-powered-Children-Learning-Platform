{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# üìñ NCTB Class 3 Bengali Book ‚Äî OCR Pipeline\n",
    "### EasyOCR ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Scanned Bengali PDF ‚Üí Unicode Text\n",
    "\n",
    "**‡¶ï‡ßá‡¶® EasyOCR?**\n",
    "- NCTB Class 3 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶á‡¶ü‡¶ø scanned image PDF\n",
    "- Standard Tesseract Bengali Unicode output ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶®‡¶æ ‡¶è‡¶á ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ PDF ‡¶è\n",
    "- EasyOCR deep learning based ‚Äî ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø accurate\n",
    "\n",
    "**Steps:**\n",
    "1. PDF ‚Üí Page images (pdftoppm)\n",
    "2. EasyOCR ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Bengali text extract\n",
    "3. Clean ‡¶ì chunk ‡¶ï‡¶∞‡¶æ\n",
    "4. FAISS vector index ‡¶§‡ßà‡¶∞‡¶ø\n",
    "5. Test retrieval\n",
    "\n",
    "---\n",
    "‚ö†Ô∏è **Setup:** PDF ‡¶ü‡¶æ Kaggle Dataset ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá add ‡¶ï‡¶∞‡ßã‡•§  \n",
    "Path ‡¶π‡¶¨‡ßá: `/kaggle/input/nctb-class3/Class-3__Bangla_combine__compressed.pdf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-install",
   "metadata": {},
   "source": ["## Step 1: Install Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core installs\n",
    "!pip install -q easyocr\n",
    "!pip install -q langchain langchain-community\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q faiss-cpu\n",
    "!apt-get install -q poppler-utils   # pdftoppm ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø\n",
    "\n",
    "print('‚úÖ ‡¶∏‡¶¨ install ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup",
   "metadata": {},
   "source": ["## Step 2: Setup ‡¶ì PDF Check"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, json, pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "PDF_PATH  = '/kaggle/input/nctb-class3/Class-3__Bangla_combine__compressed.pdf'\n",
    "WORK_DIR  = '/kaggle/working'\n",
    "IMG_DIR   = f'{WORK_DIR}/pages'      # PDF pages as PNG\n",
    "TEXT_DIR  = f'{WORK_DIR}/texts'      # Extracted text per page\n",
    "INDEX_DIR = f'{WORK_DIR}/faiss_index'  # Final FAISS index\n",
    "\n",
    "for d in [IMG_DIR, TEXT_DIR, INDEX_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# PDF exists check\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    # Kaggle input ‡¶è ‡¶®‡¶æ ‡¶™‡ßá‡¶≤‡ßá working directory ‡¶§‡ßá ‡¶ñ‡ßã‡¶Å‡¶ú‡ßã\n",
    "    alt = '/kaggle/working/Class-3__Bangla_combine__compressed.pdf'\n",
    "    if os.path.exists(alt):\n",
    "        PDF_PATH = alt\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f'PDF ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø!\\n'\n",
    "            f'Kaggle Dataset ‡¶è PDF add ‡¶ï‡¶∞‡ßã ‡¶è‡¶¨‡¶Ç path ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡ßã‡•§\\n'\n",
    "            f'Expected: {PDF_PATH}'\n",
    "        )\n",
    "\n",
    "# Page count\n",
    "result = subprocess.run(\n",
    "    ['pdfinfo', PDF_PATH], capture_output=True, text=True\n",
    ")\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if 'Pages' in line:\n",
    "        TOTAL_PAGES = int(line.split(':')[1].strip())\n",
    "        break\n",
    "\n",
    "print(f'‚úÖ PDF found: {PDF_PATH}')\n",
    "print(f'üìÑ ‡¶Æ‡ßã‡¶ü ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ: {TOTAL_PAGES}')\n",
    "print(f'üóÇÔ∏è  Working dir: {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-convert",
   "metadata": {},
   "source": ["## Step 3: PDF ‚Üí Page Images (High DPI)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pdf-to-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "DPI         = 300    # High DPI = better OCR accuracy\n",
    "START_PAGE  = 4      # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡ß© ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ cover/blank, skip ‡¶ï‡¶∞‡¶ø\n",
    "END_PAGE    = TOTAL_PAGES\n",
    "BATCH_SIZE  = 10     # ‡¶ï‡¶§‡¶ü‡¶æ page ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá convert ‡¶ï‡¶∞‡¶¨‡ßá\n",
    "\n",
    "def convert_batch(start, end, dpi=DPI):\n",
    "    \"\"\"PDF ‡¶è‡¶∞ start ‡¶•‡ßá‡¶ï‡ßá end ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ PNG ‡¶§‡ßá convert ‡¶ï‡¶∞‡¶æ‡•§\"\"\"\n",
    "    result = subprocess.run([\n",
    "        'pdftoppm', '-png', '-r', str(dpi),\n",
    "        '-f', str(start), '-l', str(end),\n",
    "        PDF_PATH, f'{IMG_DIR}/page'\n",
    "    ], capture_output=True)\n",
    "    return result.returncode == 0\n",
    "\n",
    "# Convert all pages in batches\n",
    "print(f'üîÑ Converting pages {START_PAGE}‚Äì{END_PAGE} to PNG @ {DPI} DPI...')\n",
    "print(f'   (‡¶è‡¶ü‡¶æ ‡ß´-‡ßß‡ß¶ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá)')\n",
    "\n",
    "batches = range(START_PAGE, END_PAGE + 1, BATCH_SIZE)\n",
    "for batch_start in tqdm(batches, desc='Converting batches'):\n",
    "    batch_end = min(batch_start + BATCH_SIZE - 1, END_PAGE)\n",
    "    convert_batch(batch_start, batch_end)\n",
    "\n",
    "# Count converted images\n",
    "pages = sorted(Path(IMG_DIR).glob('*.png'))\n",
    "print(f'\\n‚úÖ {len(pages)} ‡¶ü‡¶ø page image ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§')\n",
    "print(f'   Sample: {pages[0].name} ... {pages[-1].name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ocr-init",
   "metadata": {},
   "source": ["## Step 4: EasyOCR Reader Initialize"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocr-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import torch\n",
    "\n",
    "print(f'GPU available: {torch.cuda.is_available()}')\n",
    "print('üîÑ EasyOCR model load ‡¶π‡¶ö‡ßç‡¶õ‡ßá... (‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡¶¨‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá)')\n",
    "\n",
    "# Bengali + English ‡¶¶‡ßÅ‡¶ü‡ßã‡¶á recognize ‡¶ï‡¶∞‡¶¨‡ßá\n",
    "# gpu=True ‚Üí Kaggle T4 GPU ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá (‡¶Ö‡¶®‡ßá‡¶ï faster)\n",
    "READER = easyocr.Reader(\n",
    "    ['bn', 'en'],           # Bengali + English\n",
    "    gpu=torch.cuda.is_available(),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print('‚úÖ EasyOCR ready!')\n",
    "\n",
    "# Quick test on first content page\n",
    "test_img = str(pages[0])\n",
    "test_result = READER.readtext(test_img, detail=0, paragraph=True)\n",
    "print(f'\\nüß™ Quick test on {pages[0].name}:')\n",
    "print('\\n'.join(test_result[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ocr-full",
   "metadata": {},
   "source": [
    "## Step 5: Full Book OCR\n",
    "\n",
    "‡¶™‡ßÅ‡¶∞‡ßã ‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ OCR ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá‡•§  \n",
    "‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶∞ text ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ file ‡¶è save ‡¶π‡¶¨‡ßá ‚Äî resume ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ ‡¶π‡¶¨‡ßá‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocr-full",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_bengali(text):\n",
    "    \"\"\"OCR output clean ‡¶ï‡¶∞‡¶æ‡•§\"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)  # Unicode normalize\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)     # Extra blank lines ‡¶∏‡¶∞‡¶æ‡¶®‡ßã\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)         # Extra spaces ‡¶∏‡¶∞‡¶æ‡¶®‡ßã\n",
    "    return text.strip()\n",
    "\n",
    "def ocr_page(img_path):\n",
    "    \"\"\"‡¶è‡¶ï‡¶ü‡¶ø page ‡¶è‡¶∞ OCR ‡¶ï‡¶∞‡¶æ‡•§\"\"\"\n",
    "    results = READER.readtext(\n",
    "        str(img_path),\n",
    "        detail=0,\n",
    "        paragraph=True,       # Text ‡¶ï‡ßá paragraphs ‡¶è group ‡¶ï‡¶∞‡ßá\n",
    "        width_ths=0.7,        # Word grouping threshold\n",
    "        decoder='greedy'\n",
    "    )\n",
    "    return clean_bengali('\\n'.join(results))\n",
    "\n",
    "# ‚îÄ‚îÄ Run OCR on all pages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "all_page_texts = {}  # {page_num: text}\n",
    "failed_pages = []\n",
    "\n",
    "print(f'üîÑ {len(pages)} ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶∞ OCR ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "print('   (‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶Ø‡¶º ~‡ß©-‡ß´ ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°, ‡¶Æ‡ßã‡¶ü ~‡ßß‡ß¶ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü)')\n",
    "print('   Text file ‡¶ó‡ßÅ‡¶≤‡ßã automatically save ‡¶π‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡•§\\n')\n",
    "\n",
    "for img_path in tqdm(pages, desc='OCR Progress'):\n",
    "    # Page number extract ‡¶ï‡¶∞‡¶æ filename ‡¶•‡ßá‡¶ï‡ßá\n",
    "    page_num = int(img_path.stem.split('-')[-1])\n",
    "    text_file = Path(TEXT_DIR) / f'page_{page_num:03d}.txt'\n",
    "    \n",
    "    # Already processed? Skip (resume support)\n",
    "    if text_file.exists():\n",
    "        with open(text_file, encoding='utf-8') as f:\n",
    "            all_page_texts[page_num] = f.read()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        text = ocr_page(img_path)\n",
    "        all_page_texts[page_num] = text\n",
    "        \n",
    "        # Save immediately (session crash ‡¶π‡¶≤‡ßá‡¶ì progress ‡¶®‡¶∑‡ßç‡¶ü ‡¶π‡¶¨‡ßá ‡¶®‡¶æ)\n",
    "        with open(text_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è Page {page_num} failed: {e}')\n",
    "        failed_pages.append(page_num)\n",
    "        all_page_texts[page_num] = ''\n",
    "\n",
    "# Summary\n",
    "total_words = sum(len(t.split()) for t in all_page_texts.values())\n",
    "bengali_chars = sum(\n",
    "    sum(1 for c in t if '\\u0980' <= c <= '\\u09FF')\n",
    "    for t in all_page_texts.values()\n",
    ")\n",
    "\n",
    "print(f'\\n‚úÖ OCR Complete!')\n",
    "print(f'   Pages processed: {len(all_page_texts)}')\n",
    "print(f'   Total words: {total_words:,}')\n",
    "print(f'   Bengali Unicode chars: {bengali_chars:,}')\n",
    "if failed_pages:\n",
    "    print(f'   ‚ö†Ô∏è Failed pages: {failed_pages}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sample",
   "metadata": {},
   "source": ["## Step 6: Sample ‡¶¶‡ßá‡¶ñ‡¶æ"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡¶ï‡¶Ø‡¶º‡ßá‡¶ï‡¶ü‡¶æ page ‡¶è‡¶∞ text ‡¶¶‡ßá‡¶ñ‡ßã\n",
    "print('=' * 60)\n",
    "print('üìÑ SAMPLE OUTPUT ‚Äî OCR ‡¶ï‡¶∞‡¶æ Bengali Text:')\n",
    "print('=' * 60)\n",
    "\n",
    "# First few non-empty pages\n",
    "shown = 0\n",
    "for pg, text in sorted(all_page_texts.items()):\n",
    "    if text.strip() and len(text.split()) > 10:\n",
    "        print(f'\\n[‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ {pg}]')\n",
    "        print(text[:400])\n",
    "        print('---')\n",
    "        shown += 1\n",
    "        if shown >= 3:\n",
    "            break\n",
    "\n",
    "# Bengali word count check\n",
    "sample_text = list(all_page_texts.values())[5]\n",
    "bn_chars = sum(1 for c in sample_text if '\\u0980' <= c <= '\\u09FF')\n",
    "total_chars = len(sample_text)\n",
    "print(f'\\nüìä Bengali char ratio: {bn_chars}/{total_chars} = {bn_chars/max(total_chars,1)*100:.1f}%')\n",
    "\n",
    "if bn_chars / max(total_chars, 1) > 0.3:\n",
    "    print('‚úÖ Bengali text ‡¶†‡¶ø‡¶ï‡¶Æ‡¶§‡ßã extract ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá!')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Bengali chars ‡¶ï‡¶Æ ‚Äî OCR quality check ‡¶ï‡¶∞‡ßã')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-merge",
   "metadata": {},
   "source": ["## Step 7: Full Book Text ‡¶§‡ßà‡¶∞‡¶ø + Chunking"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-chunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ‡¶∏‡¶¨ page text ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶ú‡ßã‡¶°‡¶º‡¶æ ‡¶≤‡¶æ‡¶ó‡¶æ‡¶®‡ßã\n",
    "FULL_TEXT = ''\n",
    "for pg_num in sorted(all_page_texts.keys()):\n",
    "    text = all_page_texts[pg_num]\n",
    "    if text.strip():\n",
    "        FULL_TEXT += f'\\n\\n--- ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ {pg_num} ---\\n{text}'\n",
    "\n",
    "# Full text save ‡¶ï‡¶∞‡ßã\n",
    "with open(f'{WORK_DIR}/class3_bangla_full.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(FULL_TEXT)\n",
    "print(f'‚úÖ Full text saved: {len(FULL_TEXT):,} characters, {len(FULL_TEXT.split()):,} words')\n",
    "\n",
    "# ‚îÄ‚îÄ Chunking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Bengali-aware: '‡•§' (‡¶¶‡¶æ‡¶°‡¶º‡¶ø) ‡¶ï‡ßá separator ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=75,\n",
    "    separators=['\\n\\n', '\\n', '‡•§', '.', ' ', '']\n",
    ")\n",
    "\n",
    "CHUNKS = splitter.split_text(FULL_TEXT)\n",
    "\n",
    "# Empty chunks filter ‡¶ï‡¶∞‡ßã\n",
    "CHUNKS = [c.strip() for c in CHUNKS if len(c.strip().split()) >= 5]\n",
    "\n",
    "print(f'üì¶ ‡¶Æ‡ßã‡¶ü chunks: {len(CHUNKS)}')\n",
    "print(f'   Average chunk size: {sum(len(c) for c in CHUNKS) // len(CHUNKS)} chars')\n",
    "print(f'\\nüîç Sample chunk:')\n",
    "print(CHUNKS[10] if len(CHUNKS) > 10 else CHUNKS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-embed",
   "metadata": {},
   "source": ["## Step 8: Embeddings + FAISS Index"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embed-faiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "print('üîÑ Embedding model load ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "\n",
    "# Multilingual model ‚Äî Bengali, English ‡¶â‡¶≠‡¶Ø‡¶º‡¶á support ‡¶ï‡¶∞‡ßá\n",
    "EMBED_MODEL = SentenceTransformer(\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Model loaded on: {\"GPU\" if torch.cuda.is_available() else \"CPU\"}')\n",
    "print(f'üîÑ {len(CHUNKS)} chunks embed ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "\n",
    "EMBEDDINGS = EMBED_MODEL.encode(\n",
    "    CHUNKS,\n",
    "    batch_size=128,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f'\\n‚úÖ Embeddings shape: {EMBEDDINGS.shape}')\n",
    "\n",
    "# ‚îÄ‚îÄ FAISS Index ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "dim = EMBEDDINGS.shape[1]  # 384\n",
    "INDEX = faiss.IndexFlatIP(dim)  # Inner Product = Cosine similarity\n",
    "INDEX.add(EMBEDDINGS.astype('float32'))\n",
    "\n",
    "# Save everything\n",
    "faiss.write_index(INDEX, f'{INDEX_DIR}/bangla_class3.faiss')\n",
    "with open(f'{INDEX_DIR}/chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(CHUNKS, f)\n",
    "\n",
    "print(f'\\nüíæ FAISS index saved!')\n",
    "print(f'   Vectors stored: {INDEX.ntotal}')\n",
    "print(f'   Files:')\n",
    "print(f'   - {INDEX_DIR}/bangla_class3.faiss')\n",
    "print(f'   - {INDEX_DIR}/chunks.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-retrieve",
   "metadata": {},
   "source": ["## Step 9: Retrieval Test"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question, k=3):\n",
    "    \"\"\"Student ‡¶è‡¶∞ question ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø NCTB content ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶Ü‡¶®‡¶æ‡•§\"\"\"\n",
    "    q_emb = EMBED_MODEL.encode(\n",
    "        [question], normalize_embeddings=True, convert_to_numpy=True\n",
    "    ).astype('float32')\n",
    "    scores, indices = INDEX.search(q_emb, k)\n",
    "    return [\n",
    "        {'text': CHUNKS[i], 'score': float(s)}\n",
    "        for s, i in zip(scores[0], indices[0])\n",
    "        if i >= 0\n",
    "    ]\n",
    "\n",
    "# Test questions ‚Äî Bengali ‡¶ì English\n",
    "TEST_QS = [\n",
    "    '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶§‡ßá ‡¶ï‡¶§‡¶ú‡¶® ‡¶Ü‡¶õ‡ßá?',\n",
    "    '‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶ï‡ßÄ?',\n",
    "    'What is this lesson about?',\n",
    "    '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø ‡¶ï‡¶æ‡¶∞‡¶æ?',\n",
    "    '‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶ø?'\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('üß™ RETRIEVAL TEST')\n",
    "print('=' * 60)\n",
    "\n",
    "for q in TEST_QS:\n",
    "    print(f'\\n‚ùì ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {q}')\n",
    "    results = retrieve(q, k=2)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        preview = r['text'][:200].replace('\\n', ' ')\n",
    "        print(f'   [{i}] Score={r[\"score\"]:.3f}: {preview}...')\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-phi3prompt",
   "metadata": {},
   "source": ["## Step 10: Phi-3 RAG Prompt Template"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phi3-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(student_question, k=3, mode='chat'):\n",
    "    \"\"\"\n",
    "    Phi-3 ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø complete RAG prompt‡•§\n",
    "    mode: 'chat' = answer ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ, 'quiz' = MCQ generate ‡¶ï‡¶∞‡¶æ\n",
    "    \"\"\"\n",
    "    # Relevant context retrieve ‡¶ï‡¶∞‡ßã\n",
    "    chunks = retrieve(student_question, k=k)\n",
    "    context_text = '\\n\\n'.join(\n",
    "        f'[‡¶Ö‡¶Ç‡¶∂ {i+1}]:\\n{c[\"text\"]}' for i, c in enumerate(chunks)\n",
    "    )\n",
    "    \n",
    "    if mode == 'chat':\n",
    "        system = (\n",
    "            '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø AI ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï‡•§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∂‡ßç‡¶∞‡ßá‡¶£‡¶ø‡¶∞ ‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßã‡•§\\n'\n",
    "            '‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ NCTB ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∏‡¶π‡¶ú ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì‡•§\\n'\n",
    "            '‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶õ‡ßã‡¶ü ‡¶ì ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∞‡¶æ‡¶ñ‡ßã ‡¶Ø‡ßá‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡ßÆ-‡ßØ ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§'\n",
    "        )\n",
    "        user = f'‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂:\\n{context_text}\\n\\n‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {student_question}'\n",
    "        \n",
    "    elif mode == 'quiz':\n",
    "        system = (\n",
    "            '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï‡•§ NCTB ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡ßá‡¶ï‡ßá MCQ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßã‡•§\\n'\n",
    "            'Output ‡¶∂‡ßÅ‡¶ß‡ßÅ JSON format ‡¶è ‡¶¶‡¶æ‡¶ì‡•§'\n",
    "        )\n",
    "        user = (\n",
    "            f'‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂:\\n{context_text}\\n\\n'\n",
    "            f'\"{student_question}\" ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá ‡ß©‡¶ü‡¶ø ‡¶∏‡¶π‡¶ú MCQ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßã‡•§\\n'\n",
    "            'Format: {\"questions\": [{\"q\": \"...\", \"options\": [\"‡¶ï) ...\", \"‡¶ñ) ...\", \"‡¶ó) ...\", \"‡¶ò) ...\"], \"answer\": \"‡¶ï\"}]}'\n",
    "        )\n",
    "    \n",
    "    # Phi-3 prompt format\n",
    "    prompt = f'<|system|>\\n{system}\\n<|end|>\\n<|user|>\\n{user}\\n<|end|>\\n<|assistant|>'\n",
    "    return prompt\n",
    "\n",
    "# Demo\n",
    "sample_prompt = build_rag_prompt('‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶ï‡ßÄ?', mode='chat')\n",
    "print('üìù Sample Chat Prompt (Phi-3 ready):')\n",
    "print('=' * 60)\n",
    "print(sample_prompt[:800])\n",
    "print('...')\n",
    "print('=' * 60)\n",
    "\n",
    "quiz_prompt = build_rag_prompt('‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞', mode='quiz')\n",
    "print('\\nüìù Sample Quiz Prompt:')\n",
    "print('=' * 60)\n",
    "print(quiz_prompt[:600])\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": ["## ‚úÖ Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "faiss_size = os.path.getsize(f'{INDEX_DIR}/bangla_class3.faiss') / 1024\n",
    "chunks_size = os.path.getsize(f'{INDEX_DIR}/chunks.pkl') / 1024\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚úÖ NCTB Class 3 Bengali ‚Äî RAG Pipeline Complete!')\n",
    "print('=' * 60)\n",
    "print(f'''\n",
    "üìä Stats:\n",
    "   Total pages OCR'd : {len(all_page_texts)}\n",
    "   Total words       : {len(FULL_TEXT.split()):,}\n",
    "   Total chunks      : {len(CHUNKS)}\n",
    "   FAISS vectors     : {INDEX.ntotal}\n",
    "   Embedding dim     : {EMBEDDINGS.shape[1]}\n",
    "\n",
    "üíæ Saved Files:\n",
    "   {WORK_DIR}/class3_bangla_full.txt  ({os.path.getsize(f\"{WORK_DIR}/class3_bangla_full.txt\")//1024} KB)\n",
    "   {INDEX_DIR}/bangla_class3.faiss    ({faiss_size:.0f} KB)\n",
    "   {INDEX_DIR}/chunks.pkl             ({chunks_size:.0f} KB)\n",
    "   {TEXT_DIR}/page_*.txt              (per-page backups)\n",
    "\n",
    "üöÄ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ Notebook:\n",
    "   ‚Üí Phi-3 Mini inference (ollama / transformers)\n",
    "   ‚Üí Whisper STT: Bengali audio ‚Üí text\n",
    "   ‚Üí Edge-TTS: text ‚Üí Bengali voice\n",
    "   ‚Üí Django API ‡¶è integrate ‡¶ï‡¶∞‡¶æ\n",
    "''')\n",
    "print('=' * 60)"
   ]
  }
 ]
}
