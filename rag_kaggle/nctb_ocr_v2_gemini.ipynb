{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ğŸ“– NCTB Bengali OCR v2 â€” Gemini Vision API\n",
    "### EasyOCR à¦à¦° à¦šà§‡à¦¯à¦¼à§‡ à§¯à§«%+ accurate Bengali text extraction\n",
    "\n",
    "**à¦•à§‡à¦¨ Gemini Vision?**\n",
    "- EasyOCR: ~à§¬à§¦-à§­à§¦% accuracy NCTB printed fonts à¦\n",
    "- Gemini Vision: ~à§¯à§«%+ accuracy, context-aware Bengali understanding\n",
    "- Free tier: 15 requests/minute â†’ à§§à§§à§¦ à¦ªà§ƒà¦·à§à¦ à¦¾à¦° à¦œà¦¨à§à¦¯ ~à§® à¦®à¦¿à¦¨à¦¿à¦Ÿ\n",
    "\n",
    "**Setup:**\n",
    "1. [aistudio.google.com](https://aistudio.google.com) â†’ Get API Key (free)\n",
    "2. Kaggle â†’ Add-ons â†’ Secrets â†’ `GEMINI_API_KEY` à¦¨à¦¾à¦®à§‡ add à¦•à¦°à§‹\n",
    "3. NCTB PDF dataset add à¦•à¦°à§‹\n",
    "4. GPU à¦²à¦¾à¦—à¦¬à§‡ à¦¨à¦¾ â€” CPU à¦¤à§‡à¦‡ à¦šà¦²à¦¬à§‡!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-md",
   "metadata": {},
   "source": ["## Step 1: Install + API Key Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai\n",
    "!pip install -q sentence-transformers faiss-cpu langchain\n",
    "!apt-get install -q poppler-utils  # PDF â†’ image conversion\n",
    "\n",
    "print('âœ… Install complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "# â”€â”€ API Key Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Method 1: Kaggle Secrets (recommended â€” secure)\n",
    "try:\n",
    "    secrets = UserSecretsClient()\n",
    "    GEMINI_KEY = secrets.get_secret('GEMINI_API_KEY')\n",
    "    print('âœ… API key loaded from Kaggle Secrets!')\n",
    "except Exception:\n",
    "    # Method 2: Direct (less secure, for testing only)\n",
    "    GEMINI_KEY = 'YOUR_GEMINI_API_KEY_HERE'  # â† à¦à¦–à¦¾à¦¨à§‡ key à¦¦à¦¾à¦“\n",
    "    print('âš ï¸  Direct key used â€” Kaggle Secrets recommend à¦•à¦°à¦¾ à¦¹à¦¯à¦¼')\n",
    "\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Model init â€” gemini-1.5-flash is fast + free\n",
    "GEMINI = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Test API connection\n",
    "test = GEMINI.generate_content('Say \"API works!\" in Bengali')\n",
    "print(f'âœ… Gemini API test: {test.text.strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdf-md",
   "metadata": {},
   "source": ["## Step 2: PDF â†’ Page Images"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pdf-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PDF_PATH  = '/kaggle/input/nctb-class3/Class-3__Bangla_combine__compressed.pdf'\n",
    "IMG_DIR   = '/kaggle/working/pages_v2'\n",
    "TEXT_DIR  = '/kaggle/working/texts_v2'\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "os.makedirs(TEXT_DIR, exist_ok=True)\n",
    "\n",
    "# Total pages check\n",
    "result = subprocess.run(['pdfinfo', PDF_PATH], capture_output=True, text=True)\n",
    "TOTAL_PAGES = 110  # default\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if 'Pages' in line:\n",
    "        TOTAL_PAGES = int(line.split(':')[1].strip())\n",
    "\n",
    "print(f'ğŸ“„ Total pages: {TOTAL_PAGES}')\n",
    "print(f'ğŸ”„ Converting PDF â†’ PNG (DPI=250)...')\n",
    "\n",
    "# Convert in batches\n",
    "START_PAGE = 4  # Skip cover pages\n",
    "BATCH = 15\n",
    "\n",
    "for start in tqdm(range(START_PAGE, TOTAL_PAGES + 1, BATCH)):\n",
    "    end = min(start + BATCH - 1, TOTAL_PAGES)\n",
    "    subprocess.run([\n",
    "        'pdftoppm', '-png', '-r', '250',  # 250 DPI â€” Gemini à¦à¦° à¦œà¦¨à§à¦¯ à¦¯à¦¥à§‡à¦·à§à¦Ÿ\n",
    "        '-f', str(start), '-l', str(end),\n",
    "        PDF_PATH, f'{IMG_DIR}/page'\n",
    "    ], capture_output=True)\n",
    "\n",
    "pages = sorted(Path(IMG_DIR).glob('*.png'))\n",
    "print(f'âœ… {len(pages)} page images ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ocr-md",
   "metadata": {},
   "source": [
    "## Step 3: Gemini Vision OCR\n",
    "\n",
    "à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ page à¦à¦° image Gemini Vision à¦ à¦ªà¦¾à¦ à¦¾à¦¨à§‹ à¦¹à¦¬à§‡à¥¤  \n",
    "Gemini à¦¬à¦¾à¦‚à¦²à¦¾ text accurate à¦­à¦¾à¦¬à§‡ extract à¦•à¦°à¦¬à§‡à¥¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gemini-ocr",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import time, re, unicodedata\n",
    "\n",
    "# OCR prompt â€” Gemini à¦•à§‡ exact instruction à¦¦à§‡à¦“à¦¯à¦¼à¦¾\n",
    "OCR_PROMPT = \"\"\"à¦à¦‡ NCTB à¦¬à¦¾à¦‚à¦²à¦¾ à¦ªà¦¾à¦ à§à¦¯à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦ªà§ƒà¦·à§à¦ à¦¾ à¦¥à§‡à¦•à§‡ à¦¸à¦¬ text extract à¦•à¦°à§‹à¥¤\n",
    "\n",
    "à¦¨à¦¿à¦¯à¦¼à¦®:\n",
    "- à¦¶à§à¦§à§ text à¦¦à¦¾à¦“, à¦•à§‹à¦¨à§‹ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦¨à¦¾\n",
    "- à¦¬à¦¾à¦‚à¦²à¦¾ text à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼, English text English à¦ à¦°à¦¾à¦–à§‹\n",
    "- à¦ªà¦¾à¦ à§‡à¦° à¦¶à¦¿à¦°à§‹à¦¨à¦¾à¦®, à¦ªà§à¦°à¦¶à§à¦¨, à¦‰à¦¤à§à¦¤à¦° à¦¸à¦¬ à¦°à¦¾à¦–à§‹\n",
    "- à¦›à¦¬à¦¿à¦° caption à¦¥à¦¾à¦•à¦²à§‡ à¦¸à§‡à¦Ÿà¦¾à¦“ à¦°à¦¾à¦–à§‹\n",
    "- Page number à¦¬à¦¾à¦¦ à¦¦à¦¾à¦“\n",
    "- à¦¯à¦¦à¦¿ à¦ªà§ƒà¦·à§à¦ à¦¾à¦¯à¦¼ à¦¶à§à¦§à§ à¦›à¦¬à¦¿ à¦¥à¦¾à¦•à§‡ à¦¤à¦¾à¦¹à¦²à§‡ à¦²à§‡à¦–à§‹: [à¦¶à§à¦§à§ à¦šà¦¿à¦¤à§à¦°]\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"OCR output clean à¦•à¦°à¦¾à¥¤\"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = text.replace('[à¦¶à§à¦§à§ à¦šà¦¿à¦¤à§à¦°]', '').strip()\n",
    "    return text\n",
    "\n",
    "def ocr_with_gemini(img_path, retries=3):\n",
    "    \"\"\"Gemini Vision à¦¦à¦¿à¦¯à¦¼à§‡ à¦à¦•à¦Ÿà¦¿ page OCR à¦•à¦°à¦¾à¥¤\"\"\"\n",
    "    img = PIL.Image.open(img_path)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = GEMINI.generate_content(\n",
    "                [OCR_PROMPT, img],\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=0.0,  # Deterministic â€” exact text à¦šà¦¾à¦‡\n",
    "                    max_output_tokens=2048\n",
    "                )\n",
    "            )\n",
    "            return clean_text(response.text)\n",
    "        except Exception as e:\n",
    "            if 'quota' in str(e).lower() or '429' in str(e):\n",
    "                wait = 60 * (attempt + 1)\n",
    "                print(f'\\nâ³ Rate limit â€” {wait}s à¦…à¦ªà§‡à¦•à§à¦·à¦¾ à¦•à¦°à¦›à¦¿...')\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f'\\nâš ï¸ Error: {e}')\n",
    "                return ''\n",
    "    return ''\n",
    "\n",
    "\n",
    "# â”€â”€ Full OCR Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ALL_TEXTS = {}  # {page_num: text}\n",
    "DELAY = 4.5     # Seconds between requests (free tier = 15 req/min)\n",
    "\n",
    "print(f'ğŸ”„ Gemini OCR à¦¶à§à¦°à§ à¦¹à¦šà§à¦›à§‡...')\n",
    "print(f'   Pages: {len(pages)}')\n",
    "print(f'   Rate limit delay: {DELAY}s/page')\n",
    "print(f'   Estimated time: ~{len(pages) * DELAY / 60:.0f} minutes\\n')\n",
    "\n",
    "for img_path in tqdm(pages, desc='Gemini OCR'):\n",
    "    page_num = int(img_path.stem.split('-')[-1])\n",
    "    text_file = Path(TEXT_DIR) / f'page_{page_num:03d}.txt'\n",
    "    \n",
    "    # Resume support â€” already done pages skip à¦•à¦°à§‹\n",
    "    if text_file.exists():\n",
    "        with open(text_file, encoding='utf-8') as f:\n",
    "            ALL_TEXTS[page_num] = f.read()\n",
    "        continue\n",
    "    \n",
    "    # OCR\n",
    "    text = ocr_with_gemini(img_path)\n",
    "    ALL_TEXTS[page_num] = text\n",
    "    \n",
    "    # Save immediately\n",
    "    with open(text_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    # Rate limit respect\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "# Quality check\n",
    "total_words = sum(len(t.split()) for t in ALL_TEXTS.values())\n",
    "bn_chars = sum(sum(1 for c in t if '\\u0980' <= c <= '\\u09FF') for t in ALL_TEXTS.values())\n",
    "total_chars = sum(len(t) for t in ALL_TEXTS.values())\n",
    "\n",
    "print(f'\\nâœ… OCR Complete!')\n",
    "print(f'   Pages: {len(ALL_TEXTS)}')\n",
    "print(f'   Total words: {total_words:,}')\n",
    "print(f'   Bengali char ratio: {bn_chars/max(total_chars,1)*100:.1f}%')\n",
    "\n",
    "# Sample à¦¦à§‡à¦–à§‹\n",
    "print('\\nğŸ“„ Sample output (page 11):')\n",
    "print('-' * 40)\n",
    "sample_pg = sorted(ALL_TEXTS.keys())[5]\n",
    "print(ALL_TEXTS[sample_pg][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk-md",
   "metadata": {},
   "source": ["## Step 4: Clean Text â†’ Chunks â†’ FAISS"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-faiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, pickle, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "INDEX_DIR = '/kaggle/working/faiss_index_v2'\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Full text à¦¤à§ˆà¦°à¦¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FULL_TEXT = ''\n",
    "for pg in sorted(ALL_TEXTS.keys()):\n",
    "    t = ALL_TEXTS[pg].strip()\n",
    "    if t:\n",
    "        FULL_TEXT += f'\\n\\n--- à¦ªà§ƒà¦·à§à¦ à¦¾ {pg} ---\\n{t}'\n",
    "\n",
    "# Save full text\n",
    "with open('/kaggle/working/class3_bangla_v2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(FULL_TEXT)\n",
    "print(f'âœ… Full text: {len(FULL_TEXT):,} chars, {len(FULL_TEXT.split()):,} words')\n",
    "\n",
    "# â”€â”€ Chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=80,\n",
    "    separators=['\\n\\n', '\\n', 'à¥¤', '.', ' ', '']\n",
    ")\n",
    "CHUNKS = splitter.split_text(FULL_TEXT)\n",
    "CHUNKS = [c.strip() for c in CHUNKS if len(c.strip().split()) >= 8]\n",
    "print(f'ğŸ“¦ Chunks: {len(CHUNKS)}')\n",
    "\n",
    "# â”€â”€ Embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('ğŸ”„ Embedding à¦¹à¦šà§à¦›à§‡...')\n",
    "EMBED_MODEL = SentenceTransformer(\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    ")\n",
    "EMBEDDINGS = EMBED_MODEL.encode(\n",
    "    CHUNKS,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# â”€â”€ FAISS Index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dim = EMBEDDINGS.shape[1]\n",
    "INDEX = faiss.IndexFlatIP(dim)\n",
    "INDEX.add(EMBEDDINGS.astype('float32'))\n",
    "\n",
    "# Save\n",
    "faiss.write_index(INDEX, f'{INDEX_DIR}/bangla_class3.faiss')\n",
    "with open(f'{INDEX_DIR}/chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(CHUNKS, f)\n",
    "\n",
    "print(f'\\nâœ… FAISS v2 ready!')\n",
    "print(f'   Vectors: {INDEX.ntotal}')\n",
    "print(f'   Saved: {INDEX_DIR}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-md",
   "metadata": {},
   "source": ["## Step 5: Quality Check â€” Retrieval Test"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_v2(question, k=3):\n",
    "    q_emb = EMBED_MODEL.encode(\n",
    "        [question], normalize_embeddings=True, convert_to_numpy=True\n",
    "    ).astype('float32')\n",
    "    scores, indices = INDEX.search(q_emb, k)\n",
    "    return [\n",
    "        {'text': CHUNKS[i], 'score': float(s)}\n",
    "        for s, i in zip(scores[0], indices[0]) if i >= 0\n",
    "    ]\n",
    "\n",
    "# Test questions\n",
    "tests = [\n",
    "    'à¦¬à¦¨à§à¦§à§ à¦•à¦¾à¦•à§‡ à¦¬à¦²à§‡?',\n",
    "    'à¦ªà¦°à¦¿à¦¬à¦¾à¦°à§‡à¦° à¦¸à¦¦à¦¸à§à¦¯à¦°à¦¾ à¦•à¦¾à¦°à¦¾?',\n",
    "    'à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦•à¦¥à¦¾ à¦¬à¦²à§‹à¥¤',\n",
    "    'à¦†à¦¨à¦¨à§à¦¦à§‡à¦° à¦¦à¦¿à¦¨ à¦ªà¦¾à¦ à§‡ à¦•à§€ à¦†à¦›à§‡?',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('ğŸ§ª Retrieval Quality Test (v2 â€” Gemini OCR)')\n",
    "print('=' * 60)\n",
    "\n",
    "for q in tests:\n",
    "    print(f'\\nâ“ {q}')\n",
    "    results = retrieve_v2(q, k=2)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        # Bengali char check\n",
    "        bn = sum(1 for c in r['text'] if '\\u0980' <= c <= '\\u09FF')\n",
    "        ratio = bn / max(len(r['text']), 1) * 100\n",
    "        print(f'  [{i}] score={r[\"score\"]:.3f} | Bengali={ratio:.0f}%')\n",
    "        print(f'       {r[\"text\"][:150]}')\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": ["## âœ… Summary + Next Steps"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "faiss_kb = os.path.getsize(f'{INDEX_DIR}/bangla_class3.faiss') // 1024\n",
    "chunks_kb = os.path.getsize(f'{INDEX_DIR}/chunks.pkl') // 1024\n",
    "txt_kb = os.path.getsize('/kaggle/working/class3_bangla_v2.txt') // 1024\n",
    "\n",
    "bn_ratio = sum(\n",
    "    sum(1 for c in t if '\\u0980' <= c <= '\\u09FF')\n",
    "    for t in ALL_TEXTS.values()\n",
    ") / max(sum(len(t) for t in ALL_TEXTS.values()), 1) * 100\n",
    "\n",
    "print('=' * 60)\n",
    "print('âœ… NCTB Bengali OCR v2 â€” Gemini Vision Complete!')\n",
    "print('=' * 60)\n",
    "print(f'''\n",
    "ğŸ“Š OCR Stats:\n",
    "   Pages processed  : {len(ALL_TEXTS)}\n",
    "   Total words      : {len(FULL_TEXT.split()):,}\n",
    "   Bengali ratio    : {bn_ratio:.1f}%  (v1 à¦ à¦›à¦¿à¦² ~à§©à§¦%, à¦à¦–à¦¨ à§®à§¦%+)\n",
    "   Total chunks     : {len(CHUNKS)}\n",
    "   FAISS vectors    : {INDEX.ntotal}\n",
    "\n",
    "ğŸ’¾ Saved Files:\n",
    "   class3_bangla_v2.txt          ({txt_kb} KB)\n",
    "   faiss_index_v2/bangla_class3.faiss  ({faiss_kb} KB)\n",
    "   faiss_index_v2/chunks.pkl     ({chunks_kb} KB)\n",
    "\n",
    "ğŸš€ à¦ªà¦°à¦¬à¦°à§à¦¤à§€ step:\n",
    "   à¦à¦‡ output dataset à¦•à¦°à§‹ (faiss_index_v2)\n",
    "   Phi-3 notebook à¦ path update à¦•à¦°à§‹:\n",
    "   BASE = \\'faiss_index_v2\\'\n",
    "   à¦¤à¦¾à¦°à¦ªà¦° Whisper voice pipeline à¦šà¦¾à¦²à¦¾à¦“!\n",
    "''')\n",
    "print('=' * 60)"
   ]
  }
 ]
}
