{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# üß† Phi-3 RAG ‚Äî Bengali Q&A + Quiz Generation\n",
    "### NCTB Class 3 AI Tutor | Group 11 ‚Äî BUET CSE\n",
    "\n",
    "**‡¶è‡¶á notebook ‡¶è ‡¶Ø‡¶æ ‡¶π‡¶¨‡ßá:**\n",
    "1. ‡¶Ü‡¶ó‡ßá‡¶∞ notebook ‡¶è‡¶∞ FAISS index load ‡¶ï‡¶∞‡¶æ\n",
    "2. Phi-3 Mini model download ‡¶ì load ‡¶ï‡¶∞‡¶æ (Kaggle T4 GPU ‡¶§‡ßá ‡¶ö‡¶≤‡¶¨‡ßá)\n",
    "3. Bengali/English ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‚Üí NCTB context retrieve ‚Üí Phi-3 ‡¶â‡¶§‡ßç‡¶§‡¶∞\n",
    "4. AI ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Quiz generate ‡¶ï‡¶∞‡¶æ\n",
    "5. Interactive chat loop test ‡¶ï‡¶∞‡¶æ\n",
    "\n",
    "---\n",
    "**‚ö†Ô∏è Setup:**\n",
    "- ‡¶Ü‡¶ó‡ßá‡¶∞ OCR notebook ‡¶è‡¶∞ output dataset ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá add ‡¶ï‡¶∞‡ßã\n",
    "- GPU T4 x2 ON ‡¶∞‡¶æ‡¶ñ‡ßã\n",
    "- Internet ON ‡¶∞‡¶æ‡¶ñ‡ßã (model download ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-md",
   "metadata": {},
   "source": ["## Step 1: Install Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q sentence-transformers faiss-cpu\n",
    "!pip install -q langchain\n",
    "\n",
    "print('‚úÖ ‡¶∏‡¶¨ install ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-md",
   "metadata": {},
   "source": ["## Step 2: FAISS Index Load ‡¶ï‡¶∞‡¶æ (‡¶Ü‡¶ó‡ßá‡¶∞ notebook ‡¶•‡ßá‡¶ï‡ßá)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# ‡¶Ü‡¶ó‡ßá‡¶∞ notebook ‡¶è‡¶∞ output Kaggle Dataset ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá add ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá\n",
    "# Dataset name ‡¶Ø‡¶¶‡¶ø 'nctb-rag-output' ‡¶π‡¶Ø‡¶º:\n",
    "BASE = '/kaggle/input/nctb-rag-output'  # ‚Üê ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ dataset name ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ change ‡¶ï‡¶∞‡ßã\n",
    "\n",
    "# Fallback: working directory ‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶∏‡ßá‡¶ñ‡¶æ‡¶® ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶æ‡¶ì\n",
    "if not os.path.exists(BASE):\n",
    "    BASE = '/kaggle/working'\n",
    "    print('‚ö†Ô∏è  Input dataset ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø ‚Äî working directory ‡¶•‡ßá‡¶ï‡ßá load ‡¶π‡¶ö‡ßç‡¶õ‡ßá')\n",
    "\n",
    "FAISS_PATH  = f'{BASE}/faiss_index/bangla_class3.faiss'\n",
    "CHUNKS_PATH = f'{BASE}/faiss_index/chunks.pkl'\n",
    "\n",
    "# Load FAISS index\n",
    "print('üîÑ FAISS index load ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "INDEX = faiss.read_index(FAISS_PATH)\n",
    "\n",
    "# Load text chunks\n",
    "with open(CHUNKS_PATH, 'rb') as f:\n",
    "    CHUNKS = pickle.load(f)\n",
    "\n",
    "print(f'‚úÖ Index loaded!')\n",
    "print(f'   Vectors: {INDEX.ntotal}')\n",
    "print(f'   Chunks: {len(CHUNKS)}')\n",
    "print(f'   Sample chunk: {CHUNKS[5][:150]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-md",
   "metadata": {},
   "source": ["## Step 3: Embedding Model Load (Retrieval ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embed-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîÑ Embedding model load ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "\n",
    "EMBED_MODEL = SentenceTransformer(\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "def retrieve(question, k=3):\n",
    "    \"\"\"Student ‡¶è‡¶∞ question ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø relevant NCTB chunks ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶Ü‡¶®‡¶æ‡•§\"\"\"\n",
    "    q_emb = EMBED_MODEL.encode(\n",
    "        [question],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    ).astype('float32')\n",
    "    \n",
    "    scores, indices = INDEX.search(q_emb, k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx >= 0:\n",
    "            results.append({\n",
    "                'text': CHUNKS[idx],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "test = retrieve('‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡ßá?')\n",
    "print(f'\\n‚úÖ Retrieval test:')\n",
    "for i, r in enumerate(test, 1):\n",
    "    print(f'  [{i}] score={r[\"score\"]:.3f}: {r[\"text\"][:120]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-md",
   "metadata": {},
   "source": [
    "## Step 4: Phi-3 Mini Load ‡¶ï‡¶∞‡¶æ\n",
    "\n",
    "**Phi-3 Mini (3.8B)** ‚Äî Microsoft ‡¶è‡¶∞ lightweight model‡•§  \n",
    "Kaggle T4 GPU ‡¶§‡ßá 4-bit quantization ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡¶¨‡ßá (~4GB VRAM)‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phi3-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "print(f'üîÑ Phi-3 Mini load ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "print(f'   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\n",
    "print(f'   VRAM: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB')\n",
    "print('   (‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡¶¨‡¶æ‡¶∞ ~‡ß´ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá ‚Äî model download ‡¶π‡¶ö‡ßç‡¶õ‡ßá)\\n')\n",
    "\n",
    "# 4-bit quantization config ‚Äî VRAM ‡¶ï‡¶Æ ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá, speed ‡¶¨‡ßá‡¶∂‡¶ø\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Model\n",
    "PHI3 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',           # GPU/CPU automatically assign ‡¶ï‡¶∞‡¶¨‡ßá\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation='eager'  # Flash attention issue avoid ‡¶ï‡¶∞‡¶§‡ßá\n",
    ")\n",
    "\n",
    "PHI3.eval()\n",
    "print('\\n‚úÖ Phi-3 Mini ready!')\n",
    "print(f'   Memory used: {torch.cuda.memory_allocated() // 1024**2} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-md",
   "metadata": {},
   "source": ["## Step 5: RAG Generation Function"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, mode='chat', max_new_tokens=300):\n",
    "    \"\"\"\n",
    "    Student ‡¶è‡¶∞ question ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø NCTB-grounded ‡¶â‡¶§‡ßç‡¶§‡¶∞ generate ‡¶ï‡¶∞‡¶æ‡•§\n",
    "    \n",
    "    mode:\n",
    "      'chat' ‚Üí ‡¶∏‡¶π‡¶ú Bengali ‡¶â‡¶§‡ßç‡¶§‡¶∞\n",
    "      'quiz' ‚Üí MCQ questions JSON format ‡¶è\n",
    "      'explain' ‚Üí ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ\n",
    "    \"\"\"\n",
    "    # Step 1: Relevant context retrieve ‡¶ï‡¶∞‡ßã\n",
    "    chunks = retrieve(question, k=3)\n",
    "    context = '\\n\\n'.join(\n",
    "        f'[‡¶Ö‡¶Ç‡¶∂ {i+1}]: {c[\"text\"]}'\n",
    "        for i, c in enumerate(chunks)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Mode ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ prompt ‡¶§‡ßà‡¶∞‡¶ø\n",
    "    if mode == 'chat':\n",
    "        system = (\n",
    "            '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø AI ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï‡•§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∂‡ßç‡¶∞‡ßá‡¶£‡¶ø‡¶∞ ‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ '\n",
    "            'NCTB ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßã‡•§ '\n",
    "            '‡¶∏‡¶π‡¶ú, ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶õ‡ßã‡¶ü ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì‡•§ '\n",
    "            '‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂ ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì‡•§'\n",
    "        )\n",
    "        user = f'‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂:\\n{context}\\n\\n‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {question}'\n",
    "\n",
    "    elif mode == 'quiz':\n",
    "        system = (\n",
    "            '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï‡•§ NCTB ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡ßá‡¶ï‡ßá '\n",
    "            '‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∂‡ßç‡¶∞‡ßá‡¶£‡¶ø‡¶∞ ‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø MCQ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßã‡•§ '\n",
    "            '‡¶∂‡ßÅ‡¶ß‡ßÅ JSON format ‡¶è output ‡¶¶‡¶æ‡¶ì, ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶≤‡¶ø‡¶ñ‡¶¨‡ßá ‡¶®‡¶æ‡•§'\n",
    "        )\n",
    "        user = (\n",
    "            f'‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂:\\n{context}\\n\\n'\n",
    "            f'\"{question}\" ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá ‡ß©‡¶ü‡¶ø ‡¶∏‡¶π‡¶ú MCQ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßã‡•§\\n'\n",
    "            'Format:\\n'\n",
    "            '{\"questions\": [{\"question\": \"...\", '\n",
    "            '\"options\": [\"‡¶ï) ...\", \"‡¶ñ) ...\", \"‡¶ó) ...\", \"‡¶ò) ...\"], '\n",
    "            '\"correct\": \"‡¶ï\", \"explanation\": \"...\"}]}'\n",
    "        )\n",
    "\n",
    "    elif mode == 'explain':\n",
    "        system = (\n",
    "            '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø AI ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï‡•§ NCTB ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ '\n",
    "            '‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∂‡ßç‡¶∞‡ßá‡¶£‡¶ø‡¶∞ ‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ü‡¶ø ‡¶∏‡¶π‡¶ú‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßã‡•§ '\n",
    "            '‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡ßã‡¶ù‡¶æ‡¶ì‡•§'\n",
    "        )\n",
    "        user = f'‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Ö‡¶Ç‡¶∂:\\n{context}\\n\\n‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßã: {question}'\n",
    "\n",
    "    # Step 3: Phi-3 format ‡¶è prompt ‡¶§‡ßà‡¶∞‡¶ø\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system},\n",
    "        {'role': 'user',   'content': user}\n",
    "    ]\n",
    "    \n",
    "    prompt = TOKENIZER.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Step 4: Tokenize\n",
    "    inputs = TOKENIZER(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    ).to(PHI3.device)\n",
    "    \n",
    "    # Step 5: Generate\n",
    "    with torch.no_grad():\n",
    "        output = PHI3.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,     # Low temp = more focused/accurate\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=TOKENIZER.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Step 6: Decode ‚Äî ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶®‡¶§‡ßÅ‡¶® generated text\n",
    "    generated = output[0][inputs['input_ids'].shape[1]:]\n",
    "    answer = TOKENIZER.decode(generated, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return answer, context  # context ‡¶ì return ‡¶ï‡¶∞‡¶ø debug ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø\n",
    "\n",
    "\n",
    "print('‚úÖ RAG generation function ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-md",
   "metadata": {},
   "source": ["## Step 6: Bengali Q&A Test"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qa-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions ‚Äî Class 3 ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá\n",
    "test_questions = [\n",
    "    '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶ï‡ßá?',\n",
    "    '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡ßá ‡¶ï‡¶æ‡¶∞‡¶æ ‡¶•‡¶æ‡¶ï‡ßá?',\n",
    "    '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßã‡•§',\n",
    "    'What is this lesson about?',  # English test\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('üß™ Bengali Q&A Test ‚Äî Phi-3 + NCTB RAG')\n",
    "print('=' * 60)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f'\\n‚ùì ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {q}')\n",
    "    print('ü§î ‡¶â‡¶§‡ßç‡¶§‡¶∞ generate ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "    \n",
    "    answer, ctx = generate_answer(q, mode='chat')\n",
    "    \n",
    "    print(f'ü§ñ AI ‡¶â‡¶§‡ßç‡¶§‡¶∞:\\n{answer}')\n",
    "    print(f'\\nüìö Retrieved context (‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡ßß‡ß¶‡ß¶ chars): {ctx[:100]}...')\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-md",
   "metadata": {},
   "source": ["## Step 7: Quiz Generation Test"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiz-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as json_lib\n",
    "import re\n",
    "\n",
    "def generate_quiz(topic, num_q=3):\n",
    "    \"\"\"Topic ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ MCQ quiz generate ‡¶ï‡¶∞‡¶æ‡•§\"\"\"\n",
    "    answer, _ = generate_answer(topic, mode='quiz', max_new_tokens=500)\n",
    "    \n",
    "    # JSON parse ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ\n",
    "    try:\n",
    "        # JSON block extract ‡¶ï‡¶∞‡ßã\n",
    "        json_match = re.search(r'\\{.*\\}', answer, re.DOTALL)\n",
    "        if json_match:\n",
    "            quiz_data = json_lib.loads(json_match.group())\n",
    "            return quiz_data\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # JSON parse ‡¶®‡¶æ ‡¶π‡¶≤‡ßá raw text return ‡¶ï‡¶∞‡ßã\n",
    "    return {'raw': answer}\n",
    "\n",
    "\n",
    "def display_quiz(quiz_data):\n",
    "    \"\"\"Quiz ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡¶≠‡¶æ‡¶¨‡ßá display ‡¶ï‡¶∞‡¶æ‡•§\"\"\"\n",
    "    if 'raw' in quiz_data:\n",
    "        print(quiz_data['raw'])\n",
    "        return\n",
    "    \n",
    "    questions = quiz_data.get('questions', [])\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f'\\n{i}. {q.get(\"question\", \"\")}')\n",
    "        for opt in q.get('options', []):\n",
    "            print(f'   {opt}')\n",
    "        print(f'   ‚úÖ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶â‡¶§‡ßç‡¶§‡¶∞: {q.get(\"correct\", \"\")}')\n",
    "        if q.get('explanation'):\n",
    "            print(f'   üí° ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ: {q[\"explanation\"]}')\n",
    "\n",
    "\n",
    "# Test\n",
    "print('=' * 60)\n",
    "print('üìù Quiz Generation Test')\n",
    "print('=' * 60)\n",
    "\n",
    "topics = ['‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨', '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞']\n",
    "\n",
    "for topic in topics:\n",
    "    print(f'\\nüéØ Topic: {topic}')\n",
    "    print('Quiz generate ‡¶π‡¶ö‡ßç‡¶õ‡ßá...')\n",
    "    quiz = generate_quiz(topic)\n",
    "    display_quiz(quiz)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-md",
   "metadata": {},
   "source": ["## Step 8: Interactive Chat Loop (Full Pipeline Test)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"Bengali ‡¶®‡¶æ‡¶ï‡¶ø English ‡¶∏‡ßá‡¶ü‡¶æ detect ‡¶ï‡¶∞‡¶æ‡•§\"\"\"\n",
    "    bn_chars = sum(1 for c in text if '\\u0980' <= c <= '\\u09FF')\n",
    "    return 'bengali' if bn_chars > len(text) * 0.2 else 'english'\n",
    "\n",
    "def ai_tutor_response(student_input, conversation_history=[]):\n",
    "    \"\"\"\n",
    "    Full AI tutor pipeline:\n",
    "    Student input ‚Üí Language detect ‚Üí RAG ‚Üí Phi-3 ‚Üí Response\n",
    "    \"\"\"\n",
    "    lang = detect_language(student_input)\n",
    "    \n",
    "    # Quiz request detect ‡¶ï‡¶∞‡¶æ\n",
    "    quiz_keywords = ['quiz', '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßã', '‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ', 'mcq', '‡¶ï‡ßÅ‡¶á‡¶ú']\n",
    "    is_quiz = any(kw in student_input.lower() for kw in quiz_keywords)\n",
    "    \n",
    "    if is_quiz:\n",
    "        # Topic extract ‡¶ï‡¶∞‡ßã ‡¶è‡¶¨‡¶Ç quiz generate ‡¶ï‡¶∞‡ßã\n",
    "        topic = student_input.replace('quiz', '').replace('‡¶ï‡ßÅ‡¶á‡¶ú', '').replace('‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßã', '').strip()\n",
    "        if not topic:\n",
    "            topic = '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶™‡¶æ‡¶†'\n",
    "        answer, ctx = generate_answer(topic, mode='quiz', max_new_tokens=500)\n",
    "    else:\n",
    "        answer, ctx = generate_answer(student_input, mode='chat')\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'language': lang,\n",
    "        'mode': 'quiz' if is_quiz else 'chat',\n",
    "        'context_used': ctx[:200] + '...'\n",
    "    }\n",
    "\n",
    "\n",
    "# Simulate a conversation\n",
    "print('=' * 60)\n",
    "print('üí¨ AI Tutor ‚Äî Simulated Conversation')\n",
    "print('=' * 60)\n",
    "\n",
    "conversations = [\n",
    "    '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶¨‡¶≤‡ßã‡•§',\n",
    "    '‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡ßÅ‡¶á‡¶ú ‡¶¶‡¶æ‡¶ì‡•§',\n",
    "    'Tell me about family.',\n",
    "]\n",
    "\n",
    "for msg in conversations:\n",
    "    print(f'\\nüë¶ Student: {msg}')\n",
    "    response = ai_tutor_response(msg)\n",
    "    print(f'ü§ñ AI Tutor ({response[\"mode\"]} | {response[\"language\"]}):')\n",
    "    print(response['answer'])\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-md",
   "metadata": {},
   "source": ["## Step 9: Performance Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perf-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Speed test\n",
    "test_q = '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶ï‡ßÄ?'\n",
    "start = time.time()\n",
    "ans, _ = generate_answer(test_q, mode='chat')\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print('=' * 60)\n",
    "print('üìä Performance Summary')\n",
    "print('=' * 60)\n",
    "print(f'''\n",
    "üß† Model       : Phi-3 Mini 4k (4-bit quantized)\n",
    "üìö Knowledge   : NCTB Class 3 Bengali (107 pages, 190 chunks)\n",
    "üîç Retrieval   : FAISS + Multilingual MiniLM\n",
    "‚ö° Speed        : {elapsed:.1f}s per response\n",
    "üåê Languages   : Bengali + English\n",
    "üìù Modes       : Chat, Quiz, Explain\n",
    "\n",
    "‚úÖ Ready for:\n",
    "   ‚Üí Whisper STT integration (voice input)\n",
    "   ‚Üí Edge-TTS integration (voice output)\n",
    "   ‚Üí Django REST API wrapping\n",
    "   ‚Üí Flutter app connection\n",
    "''')\n",
    "print('=' * 60)"
   ]
  }
 ]
}
