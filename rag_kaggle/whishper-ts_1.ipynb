{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"kernelVersion","sourceId":298661817}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"title","cell_type":"markdown","source":"# ğŸ™ï¸ Voice Pipeline â€” Whisper STT + Phi-3.5 RAG + Edge-TTS\n### Bengali AI Tutor | Group 11 â€” BUET CSE\n\n**Full Voice Pipeline:**\n```\nğŸ¤ Bengali Audio Input\n        â†“\n   Whisper STT (OpenAI)\n        â†“\n   Language Detection\n        â†“\n   FAISS RAG (NCTB context)\n        â†“\n   Phi-3.5 Mini (answer generate)\n        â†“\n   Edge-TTS (Bengali/English voice)\n        â†“\nğŸ”Š Audio Output â†’ Student à¦¶à§à¦¨à¦¤à§‡ à¦ªà¦¾à¦¯à¦¼\n```\n\n---\n**âš ï¸ Setup:**\n- à¦†à¦—à§‡à¦° Phi-3 notebook à¦à¦° output dataset à¦¹à¦¿à¦¸à§‡à¦¬à§‡ add à¦•à¦°à§‹\n- GPU T4 ON à¦°à¦¾à¦–à§‹\n- Internet ON à¦°à¦¾à¦–à§‹","metadata":{}},{"id":"install-md","cell_type":"markdown","source":"## Step 1: Install Libraries","metadata":{}},{"id":"install","cell_type":"code","source":"# Voice pipeline libraries\n!pip install -q openai-whisper          # Whisper STT\n!pip install -q edge-tts                # Microsoft Edge TTS (Bengali voice)\n!pip install -q pydub                   # Audio processing\n!pip install -q soundfile               # Audio file read/write\n!pip install -q librosa                 # Audio analysis\n\n# RAG pipeline libraries (à¦†à¦—à§‡à¦° notebook à¦¥à§‡à¦•à§‡)\n!pip install -q sentence-transformers faiss-cpu\n!pip install -q transformers accelerate bitsandbytes\n\n# System dependencies\n!apt-get install -q ffmpeg              # Audio conversion\n\nprint('âœ… à¦¸à¦¬ install à¦¹à¦¯à¦¼à§‡à¦›à§‡!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.198697Z","iopub.status.idle":"2026-02-19T13:18:59.198908Z","shell.execute_reply.started":"2026-02-19T13:18:59.198803Z","shell.execute_reply":"2026-02-19T13:18:59.198816Z"}},"outputs":[],"execution_count":null},{"id":"load-rag-md","cell_type":"markdown","source":"## Step 2: RAG Pipeline Load (FAISS + Phi-3.5)","metadata":{}},{"id":"load-rag","cell_type":"code","source":"import os, pickle, torch\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# à¦†à¦—à§‡à¦° notebook à¦à¦° output dataset add à¦•à¦°à¦¾à¦° à¦ªà¦° path:\nBASE = '/kaggle/input/nctb-rag-output'  # â† dataset name à¦…à¦¨à§à¦¯à¦¾à¦¯à¦¼à§€ change à¦•à¦°à§‹\nif not os.path.exists(BASE):\n    BASE = '/kaggle/working'\n    print('âš ï¸ Working directory à¦¥à§‡à¦•à§‡ load à¦¹à¦šà§à¦›à§‡')\n\n# FAISS load\nprint('ğŸ”„ FAISS index load à¦¹à¦šà§à¦›à§‡...')\nINDEX = faiss.read_index(f'{BASE}/faiss_index/bangla_class3.faiss')\nwith open(f'{BASE}/faiss_index/chunks.pkl', 'rb') as f:\n    CHUNKS = pickle.load(f)\nprint(f'âœ… FAISS loaded â€” {INDEX.ntotal} vectors')\n\n# Embedding model\nprint('ğŸ”„ Embedding model load à¦¹à¦šà§à¦›à§‡...')\nEMBED_MODEL = SentenceTransformer(\n    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n)\nprint('âœ… Embedding model ready!')\n\n# Phi-3.5 load\nprint('\\nğŸ”„ Phi-3.5 Mini load à¦¹à¦šà§à¦›à§‡ (~5 min)...')\nMODEL_ID = 'microsoft/Phi-3.5-mini-instruct'\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4'\n)\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)\nPHI3 = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=bnb_config,\n    device_map='auto',\n    torch_dtype=torch.float16,\n)\nPHI3.eval()\nprint(f'âœ… Phi-3.5 ready! Memory: {torch.cuda.memory_allocated()//1024**2} MB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.199363Z","iopub.status.idle":"2026-02-19T13:18:59.199618Z","shell.execute_reply.started":"2026-02-19T13:18:59.199463Z","shell.execute_reply":"2026-02-19T13:18:59.199476Z"}},"outputs":[],"execution_count":null},{"id":"whisper-md","cell_type":"markdown","source":"## Step 3: Whisper STT Setup\n\n**Whisper** â€” OpenAI à¦à¦° speech recognition modelà¥¤  \nBengali + English à¦¦à§à¦Ÿà§‹à¦‡ recognize à¦•à¦°à§‡à¥¤  \n**medium** model â€” accuracy à¦“ speed à¦à¦° balanceà¥¤","metadata":{}},{"id":"whisper-setup","cell_type":"code","source":"import whisper\nimport numpy as np\nimport soundfile as sf\nimport tempfile\n\nprint('ğŸ”„ Whisper model load à¦¹à¦šà§à¦›à§‡...')\n# Model size options:\n# 'tiny'   â†’ fastest, less accurate\n# 'base'   â†’ good balance for simple Bengali\n# 'medium' â†’ best Bengali accuracy (recommended)\n# 'large'  â†’ most accurate but slow\nWHISPER_MODEL = whisper.load_model('medium')\nprint('âœ… Whisper medium loaded!')\n\ndef transcribe_audio(audio_path, language=None):\n    \"\"\"\n    Audio file à¦¥à§‡à¦•à§‡ text transcribe à¦•à¦°à¦¾à¥¤\n    \n    Args:\n        audio_path: audio file path (.wav, .mp3, .ogg)\n        language: 'bn' = Bengali force, None = auto detect\n    \n    Returns:\n        dict: {text, language, confidence}\n    \"\"\"\n    result = WHISPER_MODEL.transcribe(\n        audio_path,\n        language=language,        # None = auto detect\n        task='transcribe',        # 'transcribe' = same language, 'translate' = to English\n        fp16=torch.cuda.is_available(),\n        verbose=False\n    )\n    \n    detected_lang = result.get('language', 'unknown')\n    text = result['text'].strip()\n    \n    # Confidence score (average of segments)\n    segments = result.get('segments', [])\n    if segments:\n        avg_confidence = np.mean([-s.get('no_speech_prob', 0) for s in segments])\n    else:\n        avg_confidence = 0.0\n    \n    return {\n        'text': text,\n        'language': detected_lang,\n        'confidence': float(avg_confidence)\n    }\n\n# Test with a synthetic audio (silence) â€” real audio à¦¦à¦¿à¦¯à¦¼à§‡ test à¦•à¦°à¦¬à§‡\nprint('\\nâœ… Whisper STT function ready!')\nprint('   Supported: Bengali (bn), English (en), + 99 more languages')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.200647Z","iopub.status.idle":"2026-02-19T13:18:59.200985Z","shell.execute_reply.started":"2026-02-19T13:18:59.200805Z","shell.execute_reply":"2026-02-19T13:18:59.200831Z"}},"outputs":[],"execution_count":null},{"id":"tts-md","cell_type":"markdown","source":"## Step 4: Edge-TTS Setup (Bengali Voice)\n\n**Edge-TTS** â€” Microsoft à¦à¦° free TTSà¥¤  \nBengali voice: **bn-BD-NabanitaNeural** (female) à¦¬à¦¾ **bn-BD-PradeepNeural** (male)","metadata":{}},{"id":"tts-setup","cell_type":"code","source":"import edge_tts\nimport asyncio\nimport IPython.display as ipd\n\n# Available Bengali voices\nVOICES = {\n    'bengali_female': 'bn-BD-NabanitaNeural',   # à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ Bengali female\n    'bengali_male':   'bn-BD-PradeepNeural',     # à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ Bengali male\n    'english_female': 'en-US-JennyNeural',       # English fallback\n}\n\nasync def text_to_speech_async(text, voice, output_path):\n    \"\"\"Async TTS generation.\"\"\"\n    communicate = edge_tts.Communicate(text, voice)\n    await communicate.save(output_path)\n\ndef generate_speech(text, language='bengali', gender='female', output_path=None):\n    \"\"\"\n    Text à¦¥à§‡à¦•à§‡ Bengali/English speech generate à¦•à¦°à¦¾à¥¤\n    \n    Args:\n        text: à¦¬à¦²à¦¾à¦° text\n        language: 'bengali' à¦¬à¦¾ 'english'\n        gender: 'female' à¦¬à¦¾ 'male'\n        output_path: save à¦•à¦°à¦¾à¦° path (None = temp file)\n    \n    Returns:\n        audio file path\n    \"\"\"\n    if output_path is None:\n        output_path = tempfile.mktemp(suffix='.mp3')\n    \n    # Voice select à¦•à¦°à¦¾\n    if language == 'bengali':\n        voice = VOICES[f'bengali_{gender}']\n    else:\n        voice = VOICES['english_female']\n    \n    # Async run à¦•à¦°à¦¾\n    asyncio.run(text_to_speech_async(text, voice, output_path))\n    \n    return output_path\n\ndef play_audio(audio_path):\n    \"\"\"Kaggle notebook à¦ audio play à¦•à¦°à¦¾à¥¤\"\"\"\n    return ipd.Audio(audio_path, autoplay=True)\n\n# Test Bengali TTS\nprint('ğŸ”„ Bengali TTS test à¦¹à¦šà§à¦›à§‡...')\ntest_text = 'à¦†à¦®à¦¿ à¦¤à§‹à¦®à¦¾à¦° AI à¦¶à¦¿à¦•à§à¦·à¦•à¥¤ à¦¤à§‹à¦®à¦¾à¦° à¦¯à§‡à¦•à§‹à¦¨à§‹ à¦ªà§à¦°à¦¶à§à¦¨ à¦•à¦°à§‹à¥¤'\ntest_audio = generate_speech(test_text, language='bengali', gender='female')\nprint(f'âœ… Audio generated: {test_audio}')\nprint('\\nAudio play à¦¹à¦šà§à¦›à§‡:')\nplay_audio(test_audio)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.202084Z","iopub.status.idle":"2026-02-19T13:18:59.202430Z","shell.execute_reply.started":"2026-02-19T13:18:59.202255Z","shell.execute_reply":"2026-02-19T13:18:59.202281Z"}},"outputs":[],"execution_count":null},{"id":"rag-fn-md","cell_type":"markdown","source":"## Step 5: RAG + Generation Functions","metadata":{}},{"id":"rag-functions","cell_type":"code","source":"def retrieve(question, k=3):\n    \"\"\"NCTB à¦¥à§‡à¦•à§‡ relevant context retrieve à¦•à¦°à¦¾à¥¤\"\"\"\n    q_emb = EMBED_MODEL.encode(\n        [question], normalize_embeddings=True, convert_to_numpy=True\n    ).astype('float32')\n    scores, indices = INDEX.search(q_emb, k)\n    return [\n        {'text': CHUNKS[i], 'score': float(s)}\n        for s, i in zip(scores[0], indices[0]) if i >= 0\n    ]\n\ndef detect_language(text):\n    \"\"\"Bengali à¦¨à¦¾à¦•à¦¿ English detect à¦•à¦°à¦¾à¥¤\"\"\"\n    bn_chars = sum(1 for c in text if '\\u0980' <= c <= '\\u09FF')\n    return 'bengali' if bn_chars > len(text) * 0.15 else 'english'\n\ndef generate_answer(question, mode='chat', max_new_tokens=250):\n    \"\"\"RAG + Phi-3.5 à¦¦à¦¿à¦¯à¦¼à§‡ à¦‰à¦¤à§à¦¤à¦° generate à¦•à¦°à¦¾à¥¤\"\"\"\n    chunks = retrieve(question, k=3)\n    context = '\\n\\n'.join(\n        f'[à¦…à¦‚à¦¶ {i+1}]: {c[\"text\"]}' for i, c in enumerate(chunks)\n    )\n    \n    lang = detect_language(question)\n    \n    if lang == 'bengali':\n        system = (\n            'à¦¤à§à¦®à¦¿ à¦à¦•à¦Ÿà¦¿ AI à¦¶à¦¿à¦•à§à¦·à¦•à¥¤ à¦¤à§ƒà¦¤à§€à¦¯à¦¼ à¦¶à§à¦°à§‡à¦£à¦¿à¦° à¦›à¦¾à¦¤à§à¦°à¦›à¦¾à¦¤à§à¦°à§€à¦¦à§‡à¦° '\n            'NCTB à¦ªà¦¾à¦ à§à¦¯à¦¬à¦‡ à¦…à¦¨à§à¦¯à¦¾à¦¯à¦¼à§€ à¦¸à¦¹à¦œ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‹à¥¤ '\n            'à¦‰à¦¤à§à¦¤à¦° à¦›à§‹à¦Ÿ à¦“ à¦¬à¦¨à§à¦§à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦°à¦¾à¦–à§‹à¥¤'\n        )\n        user = f'à¦ªà¦¾à¦ à§à¦¯à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦…à¦‚à¦¶:\\n{context}\\n\\nà¦ªà§à¦°à¦¶à§à¦¨: {question}'\n    else:\n        system = (\n            'You are an AI tutor for Class 3 students in Bangladesh. '\n            'Answer using NCTB textbook content in simple English.'\n        )\n        user = f'Textbook content:\\n{context}\\n\\nQuestion: {question}'\n    \n    messages = [\n        {'role': 'system', 'content': system},\n        {'role': 'user',   'content': user}\n    ]\n    \n    prompt = TOKENIZER.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = TOKENIZER(\n        prompt, return_tensors='pt', truncation=True, max_length=2048\n    ).to(PHI3.device)\n    \n    with torch.no_grad():\n        output = PHI3.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.3,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            pad_token_id=TOKENIZER.eos_token_id\n        )\n    \n    generated = output[0][inputs['input_ids'].shape[1]:]\n    answer = TOKENIZER.decode(generated, skip_special_tokens=True).strip()\n    \n    return answer, lang\n\nprint('âœ… RAG + Generation functions ready!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.203521Z","iopub.status.idle":"2026-02-19T13:18:59.203859Z","shell.execute_reply.started":"2026-02-19T13:18:59.203686Z","shell.execute_reply":"2026-02-19T13:18:59.203709Z"}},"outputs":[],"execution_count":null},{"id":"pipeline-md","cell_type":"markdown","source":"## Step 6: Full Voice Pipeline\n\nà¦à¦Ÿà¦¾à¦‡ main function â€” audio in, audio outà¥¤","metadata":{}},{"id":"full-pipeline","cell_type":"code","source":"import time\n\ndef full_voice_pipeline(audio_path, tts_gender='female', verbose=True):\n    \"\"\"\n    Complete voice pipeline:\n    Audio â†’ Whisper STT â†’ RAG â†’ Phi-3.5 â†’ Edge-TTS â†’ Audio\n    \n    Args:\n        audio_path: student à¦à¦° voice recording (.wav/.mp3)\n        tts_gender: 'female' à¦¬à¦¾ 'male' AI voice\n        verbose: step by step progress à¦¦à§‡à¦–à¦¾à¦¬à§‡\n    \n    Returns:\n        dict: {question, answer, language, audio_path, timings}\n    \"\"\"\n    timings = {}\n    \n    # â”€â”€ Step 1: STT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    t0 = time.time()\n    if verbose: print('ğŸ¤ Step 1: Audio â†’ Text (Whisper)...')\n    \n    stt_result = transcribe_audio(audio_path)\n    question = stt_result['text']\n    detected_lang = stt_result['language']\n    \n    timings['stt'] = time.time() - t0\n    if verbose:\n        print(f'   âœ… Transcribed ({timings[\"stt\"]:.1f}s): \"{question}\"')\n        print(f'   ğŸŒ Language detected: {detected_lang}')\n    \n    # Empty audio check\n    if not question.strip():\n        return {'error': 'à¦•à§‹à¦¨à§‹ à¦•à¦¥à¦¾ à¦¶à§à¦¨à¦¤à§‡ à¦ªà¦¾à¦‡à¦¨à¦¿à¥¤ à¦†à¦¬à¦¾à¦° à¦¬à¦²à§‹à¥¤'}\n    \n    # â”€â”€ Step 2: RAG + LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    t1 = time.time()\n    if verbose: print('\\nğŸ§  Step 2: RAG + Phi-3.5 à¦‰à¦¤à§à¦¤à¦° generate à¦¹à¦šà§à¦›à§‡...')\n    \n    answer, answer_lang = generate_answer(question)\n    \n    timings['llm'] = time.time() - t1\n    if verbose:\n        print(f'   âœ… Answer generated ({timings[\"llm\"]:.1f}s):')\n        print(f'   ğŸ“ {answer[:200]}...')\n    \n    # â”€â”€ Step 3: TTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    t2 = time.time()\n    if verbose: print('\\nğŸ”Š Step 3: Text â†’ Bengali Audio (Edge-TTS)...')\n    \n    output_audio = tempfile.mktemp(suffix='.mp3')\n    generate_speech(\n        answer,\n        language=answer_lang,\n        gender=tts_gender,\n        output_path=output_audio\n    )\n    \n    timings['tts'] = time.time() - t2\n    timings['total'] = time.time() - t0\n    \n    if verbose:\n        print(f'   âœ… Audio generated ({timings[\"tts\"]:.1f}s)')\n        print(f'\\nâš¡ Total time: {timings[\"total\"]:.1f}s')\n        print(f'   STT: {timings[\"stt\"]:.1f}s | LLM: {timings[\"llm\"]:.1f}s | TTS: {timings[\"tts\"]:.1f}s')\n    \n    return {\n        'question': question,\n        'answer': answer,\n        'language': answer_lang,\n        'audio_path': output_audio,\n        'timings': timings\n    }\n\nprint('âœ… Full voice pipeline ready!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.204561Z","iopub.status.idle":"2026-02-19T13:18:59.204809Z","shell.execute_reply.started":"2026-02-19T13:18:59.204694Z","shell.execute_reply":"2026-02-19T13:18:59.204709Z"}},"outputs":[],"execution_count":null},{"id":"test-md","cell_type":"markdown","source":"## Step 7: Test â€” Text Input à¦¦à¦¿à¦¯à¦¼à§‡ Pipeline Test\n\nReal audio à¦¨à¦¾ à¦¥à¦¾à¦•à¦²à§‡ text à¦¥à§‡à¦•à§‡ test audio à¦¬à¦¾à¦¨à¦¿à¦¯à¦¼à§‡ test à¦•à¦°à¦¾à¥¤","metadata":{}},{"id":"test-pipeline","cell_type":"code","source":"import IPython.display as ipd\n\nasync def create_test_audio(text, path):\n    \"\"\"Test à¦à¦° à¦œà¦¨à§à¦¯ Bengali text à¦¥à§‡à¦•à§‡ audio à¦¬à¦¾à¦¨à¦¾à¦¨à§‹à¥¤\"\"\"\n    communicate = edge_tts.Communicate(text, 'bn-BD-NabanitaNeural')\n    await communicate.save(path)\n\n# Test questions\ntest_questions = [\n    'à¦¬à¦¨à§à¦§à§ à¦®à¦¾à¦¨à§‡ à¦•à§€?',\n    'à¦ªà¦°à¦¿à¦¬à¦¾à¦°à§‡ à¦•à¦¾à¦°à¦¾ à¦¥à¦¾à¦•à§‡?',\n]\n\nprint('=' * 60)\nprint('ğŸ§ª Full Voice Pipeline Test')\nprint('=' * 60)\n\nfor question in test_questions:\n    print(f'\\nâ“ Test Question: {question}')\n    \n    # Step 1: Test audio à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‹ (student à¦à¦° voice simulate)\n    test_audio_path = tempfile.mktemp(suffix='.mp3')\n    asyncio.run(create_test_audio(question, test_audio_path))\n    print(f'   ğŸ¤ Test audio created')\n    \n    # Step 2: Full pipeline run à¦•à¦°à§‹\n    result = full_voice_pipeline(test_audio_path, verbose=True)\n    \n    if 'error' not in result:\n        print(f'\\nğŸ“¢ AI à¦à¦° à¦‰à¦¤à§à¦¤à¦° (audio):')\n        display(play_audio(result['audio_path']))\n        print(f'ğŸ“ Text: {result[\"answer\"]}')\n    else:\n        print(f'âŒ Error: {result[\"error\"]}')\n    \n    print('=' * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.205892Z","iopub.status.idle":"2026-02-19T13:18:59.206134Z","shell.execute_reply.started":"2026-02-19T13:18:59.206015Z","shell.execute_reply":"2026-02-19T13:18:59.206029Z"}},"outputs":[],"execution_count":null},{"id":"realtest-md","cell_type":"markdown","source":"## Step 8: Real Audio File à¦¦à¦¿à¦¯à¦¼à§‡ Test\n\nà¦¤à§‹à¦®à¦¾à¦° à¦¨à¦¿à¦œà§‡à¦° voice recording à¦¦à¦¿à¦¯à¦¼à§‡ test à¦•à¦°à§‹à¥¤","metadata":{}},{"id":"real-audio-test","cell_type":"code","source":"# â”€â”€ Option A: Kaggle à¦ audio file upload à¦•à¦°à§‡ test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# /kaggle/input/ à¦ audio file upload à¦•à¦°à§‹\n# à¦¤à¦¾à¦°à¦ªà¦° à¦¨à¦¿à¦šà§‡à¦° path à¦¦à¦¾à¦“:\n\nMY_AUDIO = '/kaggle/input/your-audio/question.wav'  # â† à¦¤à§‹à¦®à¦¾à¦° audio path\n\nif os.path.exists(MY_AUDIO):\n    print('ğŸ¤ à¦¤à§‹à¦®à¦¾à¦° audio à¦¦à¦¿à¦¯à¦¼à§‡ test à¦¹à¦šà§à¦›à§‡...')\n    result = full_voice_pipeline(MY_AUDIO)\n    \n    print(f'\\nâ“ à¦¤à§à¦®à¦¿ à¦¬à¦²à§‡à¦›à§‹: {result[\"question\"]}')\n    print(f'ğŸ¤– AI à¦‰à¦¤à§à¦¤à¦°: {result[\"answer\"]}')\n    display(play_audio(result['audio_path']))\n\nelse:\n    print('âš ï¸  Audio file à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼à¦¨à¦¿à¥¤')\n    print('   Kaggle à¦ audio file upload à¦•à¦°à§‡ path à¦¦à¦¾à¦“à¥¤')\n    print('   à¦…à¦¥à¦¬à¦¾ Step 7 à¦à¦° test à¦¦à¦¿à¦¯à¦¼à§‡ pipeline check à¦•à¦°à§‹à¥¤')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.207585Z","iopub.status.idle":"2026-02-19T13:18:59.207936Z","shell.execute_reply.started":"2026-02-19T13:18:59.207756Z","shell.execute_reply":"2026-02-19T13:18:59.207780Z"}},"outputs":[],"execution_count":null},{"id":"django-md","cell_type":"markdown","source":"## Step 9: Django API à¦à¦° à¦œà¦¨à§à¦¯ Pipeline Function\n\nà¦à¦‡ function à¦Ÿà¦¾ Django view à¦ directly use à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡à¥¤","metadata":{}},{"id":"django-ready","cell_type":"code","source":"import base64\n\ndef process_voice_request(audio_bytes, session_id=None, gender='female'):\n    \"\"\"\n    Django REST API request handle à¦•à¦°à¦¾à¦° functionà¥¤\n    \n    Input:  audio bytes (Flutter à¦¥à§‡à¦•à§‡ à¦†à¦¸à¦¬à§‡)\n    Output: JSON response (text + audio base64)\n    \n    Django view à¦ à¦à¦­à¦¾à¦¬à§‡ use à¦•à¦°à¦¬à§‡:\n    \n    @api_view(['POST'])\n    def ai_tutor_chat(request):\n        audio_bytes = request.data['audio']\n        response = process_voice_request(audio_bytes)\n        return Response(response)\n    \"\"\"\n    # Audio bytes â†’ temp file\n    audio_path = tempfile.mktemp(suffix='.wav')\n    with open(audio_path, 'wb') as f:\n        if isinstance(audio_bytes, str):  # base64 string\n            f.write(base64.b64decode(audio_bytes))\n        else:  # raw bytes\n            f.write(audio_bytes)\n    \n    # Pipeline run à¦•à¦°à§‹\n    result = full_voice_pipeline(audio_path, tts_gender=gender, verbose=False)\n    \n    if 'error' in result:\n        return {\n            'success': False,\n            'error': result['error']\n        }\n    \n    # Response audio â†’ base64 (Flutter à¦ à¦ªà¦¾à¦ à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯)\n    with open(result['audio_path'], 'rb') as f:\n        audio_base64 = base64.b64encode(f.read()).decode('utf-8')\n    \n    return {\n        'success': True,\n        'session_id': session_id,\n        'question': result['question'],           # Whisper transcription\n        'answer_text': result['answer'],          # Phi-3.5 answer\n        'answer_audio': audio_base64,             # Edge-TTS audio (base64)\n        'language': result['language'],           # 'bengali' à¦¬à¦¾ 'english'\n        'timings': result['timings']             # Performance metrics\n    }\n\nprint('âœ… Django-ready API function à¦¤à§ˆà¦°à¦¿!')\nprint('\\nğŸ“‹ API Response Format:')\nprint('''\n{\n  \"success\": true,\n  \"session_id\": \"...\",\n  \"question\": \"à¦¬à¦¨à§à¦§à§ à¦®à¦¾à¦¨à§‡ à¦•à§€?\",          â† Whisper STT\n  \"answer_text\": \"à¦¬à¦¨à§à¦§à§ à¦¹à¦²à§‹...\",         â† Phi-3.5 RAG\n  \"answer_audio\": \"base64...\",           â† Edge-TTS\n  \"language\": \"bengali\",\n  \"timings\": {\"stt\": 2.1, \"llm\": 7.5, \"tts\": 1.2, \"total\": 10.8}\n}\n''')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.209317Z","iopub.status.idle":"2026-02-19T13:18:59.210052Z","shell.execute_reply.started":"2026-02-19T13:18:59.209908Z","shell.execute_reply":"2026-02-19T13:18:59.209932Z"}},"outputs":[],"execution_count":null},{"id":"summary-md","cell_type":"markdown","source":"## âœ… Summary","metadata":{}},{"id":"summary","cell_type":"code","source":"print('=' * 60)\nprint('âœ… Voice Pipeline â€” Complete!')\nprint('=' * 60)\nprint('''\nğŸ¤ STT  : Whisper Medium â€” Bengali + English\nğŸ§  LLM  : Phi-3.5 Mini 4-bit â€” NCTB RAG\nğŸ”Š TTS  : Edge-TTS â€” bn-BD-NabanitaNeural\nâš¡ Speed : ~10-12s total per response\n\nExpected latency breakdown:\n  Whisper STT  : ~2-3s\n  Phi-3.5 RAG  : ~7-8s  \n  Edge-TTS     : ~1-2s\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Total        : ~10-13s\n\nğŸš€ à¦ªà¦°à¦¬à¦°à§à¦¤à§€ step:\n  â†’ Django REST API à¦¤à§‡ integrate à¦•à¦°à¦¾\n  â†’ Flutter app à¦¥à§‡à¦•à§‡ audio send/receive\n  â†’ Frustration detection add à¦•à¦°à¦¾\n  â†’ Quiz mode integrate à¦•à¦°à¦¾\n''')\nprint('=' * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T13:18:59.212252Z","iopub.status.idle":"2026-02-19T13:18:59.212914Z","shell.execute_reply.started":"2026-02-19T13:18:59.212742Z","shell.execute_reply":"2026-02-19T13:18:59.212768Z"}},"outputs":[],"execution_count":null}]}