{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ğŸ™ï¸ Voice Pipeline â€” Whisper STT + Phi-3.5 RAG + Edge-TTS\n",
    "### Bengali AI Tutor | Group 11 â€” BUET CSE\n",
    "\n",
    "**Full Voice Pipeline:**\n",
    "```\n",
    "ğŸ¤ Bengali Audio Input\n",
    "        â†“\n",
    "   Whisper STT (OpenAI)\n",
    "        â†“\n",
    "   Language Detection\n",
    "        â†“\n",
    "   FAISS RAG (NCTB context)\n",
    "        â†“\n",
    "   Phi-3.5 Mini (answer generate)\n",
    "        â†“\n",
    "   Edge-TTS (Bengali/English voice)\n",
    "        â†“\n",
    "ğŸ”Š Audio Output â†’ Student à¦¶à§à¦¨à¦¤à§‡ à¦ªà¦¾à¦¯à¦¼\n",
    "```\n",
    "\n",
    "---\n",
    "**âš ï¸ Setup:**\n",
    "- à¦†à¦—à§‡à¦° Phi-3 notebook à¦à¦° output dataset à¦¹à¦¿à¦¸à§‡à¦¬à§‡ add à¦•à¦°à§‹\n",
    "- GPU T4 ON à¦°à¦¾à¦–à§‹\n",
    "- Internet ON à¦°à¦¾à¦–à§‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-md",
   "metadata": {},
   "source": ["## Step 1: Install Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice pipeline libraries\n",
    "!pip install -q openai-whisper          # Whisper STT\n",
    "!pip install -q edge-tts                # Microsoft Edge TTS (Bengali voice)\n",
    "!pip install -q pydub                   # Audio processing\n",
    "!pip install -q soundfile               # Audio file read/write\n",
    "!pip install -q librosa                 # Audio analysis\n",
    "\n",
    "# RAG pipeline libraries (à¦†à¦—à§‡à¦° notebook à¦¥à§‡à¦•à§‡)\n",
    "!pip install -q sentence-transformers faiss-cpu\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "\n",
    "# System dependencies\n",
    "!apt-get install -q ffmpeg              # Audio conversion\n",
    "\n",
    "print('âœ… à¦¸à¦¬ install à¦¹à¦¯à¦¼à§‡à¦›à§‡!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-rag-md",
   "metadata": {},
   "source": ["## Step 2: RAG Pipeline Load (FAISS + Phi-3.5)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-rag",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# à¦†à¦—à§‡à¦° notebook à¦à¦° output dataset add à¦•à¦°à¦¾à¦° à¦ªà¦° path:\n",
    "BASE = '/kaggle/input/nctb-rag-output'  # â† dataset name à¦…à¦¨à§à¦¯à¦¾à¦¯à¦¼à§€ change à¦•à¦°à§‹\n",
    "if not os.path.exists(BASE):\n",
    "    BASE = '/kaggle/working'\n",
    "    print('âš ï¸ Working directory à¦¥à§‡à¦•à§‡ load à¦¹à¦šà§à¦›à§‡')\n",
    "\n",
    "# FAISS load\n",
    "print('ğŸ”„ FAISS index load à¦¹à¦šà§à¦›à§‡...')\n",
    "INDEX = faiss.read_index(f'{BASE}/faiss_index/bangla_class3.faiss')\n",
    "with open(f'{BASE}/faiss_index/chunks.pkl', 'rb') as f:\n",
    "    CHUNKS = pickle.load(f)\n",
    "print(f'âœ… FAISS loaded â€” {INDEX.ntotal} vectors')\n",
    "\n",
    "# Embedding model\n",
    "print('ğŸ”„ Embedding model load à¦¹à¦šà§à¦›à§‡...')\n",
    "EMBED_MODEL = SentenceTransformer(\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print('âœ… Embedding model ready!')\n",
    "\n",
    "# Phi-3.5 load\n",
    "print('\\nğŸ”„ Phi-3.5 Mini load à¦¹à¦šà§à¦›à§‡ (~5 min)...')\n",
    "MODEL_ID = 'microsoft/Phi-3.5-mini-instruct'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "PHI3 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "PHI3.eval()\n",
    "print(f'âœ… Phi-3.5 ready! Memory: {torch.cuda.memory_allocated()//1024**2} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whisper-md",
   "metadata": {},
   "source": [
    "## Step 3: Whisper STT Setup\n",
    "\n",
    "**Whisper** â€” OpenAI à¦à¦° speech recognition modelà¥¤  \n",
    "Bengali + English à¦¦à§à¦Ÿà§‹à¦‡ recognize à¦•à¦°à§‡à¥¤  \n",
    "**medium** model â€” accuracy à¦“ speed à¦à¦° balanceà¥¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whisper-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "\n",
    "print('ğŸ”„ Whisper model load à¦¹à¦šà§à¦›à§‡...')\n",
    "# Model size options:\n",
    "# 'tiny'   â†’ fastest, less accurate\n",
    "# 'base'   â†’ good balance for simple Bengali\n",
    "# 'medium' â†’ best Bengali accuracy (recommended)\n",
    "# 'large'  â†’ most accurate but slow\n",
    "WHISPER_MODEL = whisper.load_model('medium')\n",
    "print('âœ… Whisper medium loaded!')\n",
    "\n",
    "def transcribe_audio(audio_path, language=None):\n",
    "    \"\"\"\n",
    "    Audio file à¦¥à§‡à¦•à§‡ text transcribe à¦•à¦°à¦¾à¥¤\n",
    "    \n",
    "    Args:\n",
    "        audio_path: audio file path (.wav, .mp3, .ogg)\n",
    "        language: 'bn' = Bengali force, None = auto detect\n",
    "    \n",
    "    Returns:\n",
    "        dict: {text, language, confidence}\n",
    "    \"\"\"\n",
    "    result = WHISPER_MODEL.transcribe(\n",
    "        audio_path,\n",
    "        language=language,        # None = auto detect\n",
    "        task='transcribe',        # 'transcribe' = same language, 'translate' = to English\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    detected_lang = result.get('language', 'unknown')\n",
    "    text = result['text'].strip()\n",
    "    \n",
    "    # Confidence score (average of segments)\n",
    "    segments = result.get('segments', [])\n",
    "    if segments:\n",
    "        avg_confidence = np.mean([-s.get('no_speech_prob', 0) for s in segments])\n",
    "    else:\n",
    "        avg_confidence = 0.0\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'language': detected_lang,\n",
    "        'confidence': float(avg_confidence)\n",
    "    }\n",
    "\n",
    "# Test with a synthetic audio (silence) â€” real audio à¦¦à¦¿à¦¯à¦¼à§‡ test à¦•à¦°à¦¬à§‡\n",
    "print('\\nâœ… Whisper STT function ready!')\n",
    "print('   Supported: Bengali (bn), English (en), + 99 more languages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tts-md",
   "metadata": {},
   "source": [
    "## Step 4: Edge-TTS Setup (Bengali Voice)\n",
    "\n",
    "**Edge-TTS** â€” Microsoft à¦à¦° free TTSà¥¤  \n",
    "Bengali voice: **bn-BD-NabanitaNeural** (female) à¦¬à¦¾ **bn-BD-PradeepNeural** (male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import edge_tts\n",
    "import asyncio\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Available Bengali voices\n",
    "VOICES = {\n",
    "    'bengali_female': 'bn-BD-NabanitaNeural',   # à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ Bengali female\n",
    "    'bengali_male':   'bn-BD-PradeepNeural',     # à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ Bengali male\n",
    "    'english_female': 'en-US-JennyNeural',       # English fallback\n",
    "}\n",
    "\n",
    "async def text_to_speech_async(text, voice, output_path):\n",
    "    \"\"\"Async TTS generation.\"\"\"\n",
    "    communicate = edge_tts.Communicate(text, voice)\n",
    "    await communicate.save(output_path)\n",
    "\n",
    "def generate_speech(text, language='bengali', gender='female', output_path=None):\n",
    "    \"\"\"\n",
    "    Text à¦¥à§‡à¦•à§‡ Bengali/English speech generate à¦•à¦°à¦¾à¥¤\n",
    "    \n",
    "    Args:\n",
    "        text: à¦¬à¦²à¦¾à¦° text\n",
    "        language: 'bengali' à¦¬à¦¾ 'english'\n",
    "        gender: 'female' à¦¬à¦¾ 'male'\n",
    "        output_path: save à¦•à¦°à¦¾à¦° path (None = temp file)\n",
    "    \n",
    "    Returns:\n",
    "        audio file path\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = tempfile.mktemp(suffix='.mp3')\n",
    "    \n",
    "    # Voice select à¦•à¦°à¦¾\n",
    "    if language == 'bengali':\n",
    "        voice = VOICES[f'bengali_{gender}']\n",
    "    else:\n",
    "        voice = VOICES['english_female']\n",
    "    \n",
    "    # Async run à¦•à¦°à¦¾\n",
    "    asyncio.run(text_to_speech_async(text, voice, output_path))\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def play_audio(audio_path):\n",
    "    \"\"\"Kaggle notebook à¦ audio play à¦•à¦°à¦¾à¥¤\"\"\"\n",
    "    return ipd.Audio(audio_path, autoplay=True)\n",
    "\n",
    "# Test Bengali TTS\n",
    "print('ğŸ”„ Bengali TTS test à¦¹à¦šà§à¦›à§‡...')\n",
    "test_text = 'à¦†à¦®à¦¿ à¦¤à§‹à¦®à¦¾à¦° AI à¦¶à¦¿à¦•à§à¦·à¦•à¥¤ à¦¤à§‹à¦®à¦¾à¦° à¦¯à§‡à¦•à§‹à¦¨à§‹ à¦ªà§à¦°à¦¶à§à¦¨ à¦•à¦°à§‹à¥¤'\n",
    "test_audio = generate_speech(test_text, language='bengali', gender='female')\n",
    "print(f'âœ… Audio generated: {test_audio}')\n",
    "print('\\nAudio play à¦¹à¦šà§à¦›à§‡:')\n",
    "play_audio(test_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-fn-md",
   "metadata": {},
   "source": ["## Step 5: RAG + Generation Functions"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question, k=3):\n",
    "    \"\"\"NCTB à¦¥à§‡à¦•à§‡ relevant context retrieve à¦•à¦°à¦¾à¥¤\"\"\"\n",
    "    q_emb = EMBED_MODEL.encode(\n",
    "        [question], normalize_embeddings=True, convert_to_numpy=True\n",
    "    ).astype('float32')\n",
    "    scores, indices = INDEX.search(q_emb, k)\n",
    "    return [\n",
    "        {'text': CHUNKS[i], 'score': float(s)}\n",
    "        for s, i in zip(scores[0], indices[0]) if i >= 0\n",
    "    ]\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Bengali à¦¨à¦¾à¦•à¦¿ English detect à¦•à¦°à¦¾à¥¤\"\"\"\n",
    "    bn_chars = sum(1 for c in text if '\\u0980' <= c <= '\\u09FF')\n",
    "    return 'bengali' if bn_chars > len(text) * 0.15 else 'english'\n",
    "\n",
    "def generate_answer(question, mode='chat', max_new_tokens=250):\n",
    "    \"\"\"RAG + Phi-3.5 à¦¦à¦¿à¦¯à¦¼à§‡ à¦‰à¦¤à§à¦¤à¦° generate à¦•à¦°à¦¾à¥¤\"\"\"\n",
    "    chunks = retrieve(question, k=3)\n",
    "    context = '\\n\\n'.join(\n",
    "        f'[à¦…à¦‚à¦¶ {i+1}]: {c[\"text\"]}' for i, c in enumerate(chunks)\n",
    "    )\n",
    "    \n",
    "    lang = detect_language(question)\n",
    "    \n",
    "    if lang == 'bengali':\n",
    "        system = (\n",
    "            'à¦¤à§à¦®à¦¿ à¦à¦•à¦Ÿà¦¿ AI à¦¶à¦¿à¦•à§à¦·à¦•à¥¤ à¦¤à§ƒà¦¤à§€à¦¯à¦¼ à¦¶à§à¦°à§‡à¦£à¦¿à¦° à¦›à¦¾à¦¤à§à¦°à¦›à¦¾à¦¤à§à¦°à§€à¦¦à§‡à¦° '\n",
    "            'NCTB à¦ªà¦¾à¦ à§à¦¯à¦¬à¦‡ à¦…à¦¨à§à¦¯à¦¾à¦¯à¦¼à§€ à¦¸à¦¹à¦œ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‹à¥¤ '\n",
    "            'à¦‰à¦¤à§à¦¤à¦° à¦›à§‹à¦Ÿ à¦“ à¦¬à¦¨à§à¦§à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦°à¦¾à¦–à§‹à¥¤'\n",
    "        )\n",
    "        user = f'à¦ªà¦¾à¦ à§à¦¯à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦…à¦‚à¦¶:\\n{context}\\n\\nà¦ªà§à¦°à¦¶à§à¦¨: {question}'\n",
    "    else:\n",
    "        system = (\n",
    "            'You are an AI tutor for Class 3 students in Bangladesh. '\n",
    "            'Answer using NCTB textbook content in simple English.'\n",
    "        )\n",
    "        user = f'Textbook content:\\n{context}\\n\\nQuestion: {question}'\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system},\n",
    "        {'role': 'user',   'content': user}\n",
    "    ]\n",
    "    \n",
    "    prompt = TOKENIZER.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = TOKENIZER(\n",
    "        prompt, return_tensors='pt', truncation=True, max_length=2048\n",
    "    ).to(PHI3.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = PHI3.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=TOKENIZER.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated = output[0][inputs['input_ids'].shape[1]:]\n",
    "    answer = TOKENIZER.decode(generated, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return answer, lang\n",
    "\n",
    "print('âœ… RAG + Generation functions ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-md",
   "metadata": {},
   "source": [
    "## Step 6: Full Voice Pipeline\n",
    "\n",
    "à¦à¦Ÿà¦¾à¦‡ main function â€” audio in, audio outà¥¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def full_voice_pipeline(audio_path, tts_gender='female', verbose=True):\n",
    "    \"\"\"\n",
    "    Complete voice pipeline:\n",
    "    Audio â†’ Whisper STT â†’ RAG â†’ Phi-3.5 â†’ Edge-TTS â†’ Audio\n",
    "    \n",
    "    Args:\n",
    "        audio_path: student à¦à¦° voice recording (.wav/.mp3)\n",
    "        tts_gender: 'female' à¦¬à¦¾ 'male' AI voice\n",
    "        verbose: step by step progress à¦¦à§‡à¦–à¦¾à¦¬à§‡\n",
    "    \n",
    "    Returns:\n",
    "        dict: {question, answer, language, audio_path, timings}\n",
    "    \"\"\"\n",
    "    timings = {}\n",
    "    \n",
    "    # â”€â”€ Step 1: STT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    t0 = time.time()\n",
    "    if verbose: print('ğŸ¤ Step 1: Audio â†’ Text (Whisper)...')\n",
    "    \n",
    "    stt_result = transcribe_audio(audio_path)\n",
    "    question = stt_result['text']\n",
    "    detected_lang = stt_result['language']\n",
    "    \n",
    "    timings['stt'] = time.time() - t0\n",
    "    if verbose:\n",
    "        print(f'   âœ… Transcribed ({timings[\"stt\"]:.1f}s): \"{question}\"')\n",
    "        print(f'   ğŸŒ Language detected: {detected_lang}')\n",
    "    \n",
    "    # Empty audio check\n",
    "    if not question.strip():\n",
    "        return {'error': 'à¦•à§‹à¦¨à§‹ à¦•à¦¥à¦¾ à¦¶à§à¦¨à¦¤à§‡ à¦ªà¦¾à¦‡à¦¨à¦¿à¥¤ à¦†à¦¬à¦¾à¦° à¦¬à¦²à§‹à¥¤'}\n",
    "    \n",
    "    # â”€â”€ Step 2: RAG + LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    t1 = time.time()\n",
    "    if verbose: print('\\nğŸ§  Step 2: RAG + Phi-3.5 à¦‰à¦¤à§à¦¤à¦° generate à¦¹à¦šà§à¦›à§‡...')\n",
    "    \n",
    "    answer, answer_lang = generate_answer(question)\n",
    "    \n",
    "    timings['llm'] = time.time() - t1\n",
    "    if verbose:\n",
    "        print(f'   âœ… Answer generated ({timings[\"llm\"]:.1f}s):')\n",
    "        print(f'   ğŸ“ {answer[:200]}...')\n",
    "    \n",
    "    # â”€â”€ Step 3: TTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    t2 = time.time()\n",
    "    if verbose: print('\\nğŸ”Š Step 3: Text â†’ Bengali Audio (Edge-TTS)...')\n",
    "    \n",
    "    output_audio = tempfile.mktemp(suffix='.mp3')\n",
    "    generate_speech(\n",
    "        answer,\n",
    "        language=answer_lang,\n",
    "        gender=tts_gender,\n",
    "        output_path=output_audio\n",
    "    )\n",
    "    \n",
    "    timings['tts'] = time.time() - t2\n",
    "    timings['total'] = time.time() - t0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'   âœ… Audio generated ({timings[\"tts\"]:.1f}s)')\n",
    "        print(f'\\nâš¡ Total time: {timings[\"total\"]:.1f}s')\n",
    "        print(f'   STT: {timings[\"stt\"]:.1f}s | LLM: {timings[\"llm\"]:.1f}s | TTS: {timings[\"tts\"]:.1f}s')\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'language': answer_lang,\n",
    "        'audio_path': output_audio,\n",
    "        'timings': timings\n",
    "    }\n",
    "\n",
    "print('âœ… Full voice pipeline ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-md",
   "metadata": {},
   "source": [
    "## Step 7: Test â€” Text Input à¦¦à¦¿à¦¯à¦¼à§‡ Pipeline Test\n",
    "\n",
    "Real audio à¦¨à¦¾ à¦¥à¦¾à¦•à¦²à§‡ text à¦¥à§‡à¦•à§‡ test audio à¦¬à¦¾à¦¨à¦¿à¦¯à¦¼à§‡ test à¦•à¦°à¦¾à¥¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "async def create_test_audio(text, path):\n",
    "    \"\"\"Test à¦à¦° à¦œà¦¨à§à¦¯ Bengali text à¦¥à§‡à¦•à§‡ audio à¦¬à¦¾à¦¨à¦¾à¦¨à§‹à¥¤\"\"\"\n",
    "    communicate = edge_tts.Communicate(text, 'bn-BD-NabanitaNeural')\n",
    "    await communicate.save(path)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    'à¦¬à¦¨à§à¦§à§ à¦®à¦¾à¦¨à§‡ à¦•à§€?',\n",
    "    'à¦ªà¦°à¦¿à¦¬à¦¾à¦°à§‡ à¦•à¦¾à¦°à¦¾ à¦¥à¦¾à¦•à§‡?',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('ğŸ§ª Full Voice Pipeline Test')\n",
    "print('=' * 60)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f'\\nâ“ Test Question: {question}')\n",
    "    \n",
    "    # Step 1: Test audio à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‹ (student à¦à¦° voice simulate)\n",
    "    test_audio_path = tempfile.mktemp(suffix='.mp3')\n",
    "    asyncio.run(create_test_audio(question, test_audio_path))\n",
    "    print(f'   ğŸ¤ Test audio created')\n",
    "    \n",
    "    # Step 2: Full pipeline run à¦•à¦°à§‹\n",
    "    result = full_voice_pipeline(test_audio_path, verbose=True)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f'\\nğŸ“¢ AI à¦à¦° à¦‰à¦¤à§à¦¤à¦° (audio):')\n",
    "        display(play_audio(result['audio_path']))\n",
    "        print(f'ğŸ“ Text: {result[\"answer\"]}')\n",
    "    else:\n",
    "        print(f'âŒ Error: {result[\"error\"]}')\n",
    "    \n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realtest-md",
   "metadata": {},
   "source": [
    "## Step 8: Real Audio File à¦¦à¦¿à¦¯à¦¼à§‡ Test\n",
    "\n",
    "à¦¤à§‹à¦®à¦¾à¦° à¦¨à¦¿à¦œà§‡à¦° voice recording à¦¦à¦¿à¦¯à¦¼à§‡ test à¦•à¦°à§‹à¥¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "real-audio-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Option A: Kaggle à¦ audio file upload à¦•à¦°à§‡ test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# /kaggle/input/ à¦ audio file upload à¦•à¦°à§‹\n",
    "# à¦¤à¦¾à¦°à¦ªà¦° à¦¨à¦¿à¦šà§‡à¦° path à¦¦à¦¾à¦“:\n",
    "\n",
    "MY_AUDIO = '/kaggle/input/your-audio/question.wav'  # â† à¦¤à§‹à¦®à¦¾à¦° audio path\n",
    "\n",
    "if os.path.exists(MY_AUDIO):\n",
    "    print('ğŸ¤ à¦¤à§‹à¦®à¦¾à¦° audio à¦¦à¦¿à¦¯à¦¼à§‡ test à¦¹à¦šà§à¦›à§‡...')\n",
    "    result = full_voice_pipeline(MY_AUDIO)\n",
    "    \n",
    "    print(f'\\nâ“ à¦¤à§à¦®à¦¿ à¦¬à¦²à§‡à¦›à§‹: {result[\"question\"]}')\n",
    "    print(f'ğŸ¤– AI à¦‰à¦¤à§à¦¤à¦°: {result[\"answer\"]}')\n",
    "    display(play_audio(result['audio_path']))\n",
    "\n",
    "else:\n",
    "    print('âš ï¸  Audio file à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼à¦¨à¦¿à¥¤')\n",
    "    print('   Kaggle à¦ audio file upload à¦•à¦°à§‡ path à¦¦à¦¾à¦“à¥¤')\n",
    "    print('   à¦…à¦¥à¦¬à¦¾ Step 7 à¦à¦° test à¦¦à¦¿à¦¯à¦¼à§‡ pipeline check à¦•à¦°à§‹à¥¤')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "django-md",
   "metadata": {},
   "source": [
    "## Step 9: Django API à¦à¦° à¦œà¦¨à§à¦¯ Pipeline Function\n",
    "\n",
    "à¦à¦‡ function à¦Ÿà¦¾ Django view à¦ directly use à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡à¥¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "django-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def process_voice_request(audio_bytes, session_id=None, gender='female'):\n",
    "    \"\"\"\n",
    "    Django REST API request handle à¦•à¦°à¦¾à¦° functionà¥¤\n",
    "    \n",
    "    Input:  audio bytes (Flutter à¦¥à§‡à¦•à§‡ à¦†à¦¸à¦¬à§‡)\n",
    "    Output: JSON response (text + audio base64)\n",
    "    \n",
    "    Django view à¦ à¦à¦­à¦¾à¦¬à§‡ use à¦•à¦°à¦¬à§‡:\n",
    "    \n",
    "    @api_view(['POST'])\n",
    "    def ai_tutor_chat(request):\n",
    "        audio_bytes = request.data['audio']\n",
    "        response = process_voice_request(audio_bytes)\n",
    "        return Response(response)\n",
    "    \"\"\"\n",
    "    # Audio bytes â†’ temp file\n",
    "    audio_path = tempfile.mktemp(suffix='.wav')\n",
    "    with open(audio_path, 'wb') as f:\n",
    "        if isinstance(audio_bytes, str):  # base64 string\n",
    "            f.write(base64.b64decode(audio_bytes))\n",
    "        else:  # raw bytes\n",
    "            f.write(audio_bytes)\n",
    "    \n",
    "    # Pipeline run à¦•à¦°à§‹\n",
    "    result = full_voice_pipeline(audio_path, tts_gender=gender, verbose=False)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': result['error']\n",
    "        }\n",
    "    \n",
    "    # Response audio â†’ base64 (Flutter à¦ à¦ªà¦¾à¦ à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯)\n",
    "    with open(result['audio_path'], 'rb') as f:\n",
    "        audio_base64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'session_id': session_id,\n",
    "        'question': result['question'],           # Whisper transcription\n",
    "        'answer_text': result['answer'],          # Phi-3.5 answer\n",
    "        'answer_audio': audio_base64,             # Edge-TTS audio (base64)\n",
    "        'language': result['language'],           # 'bengali' à¦¬à¦¾ 'english'\n",
    "        'timings': result['timings']             # Performance metrics\n",
    "    }\n",
    "\n",
    "print('âœ… Django-ready API function à¦¤à§ˆà¦°à¦¿!')\n",
    "print('\\nğŸ“‹ API Response Format:')\n",
    "print('''\n",
    "{\n",
    "  \"success\": true,\n",
    "  \"session_id\": \"...\",\n",
    "  \"question\": \"à¦¬à¦¨à§à¦§à§ à¦®à¦¾à¦¨à§‡ à¦•à§€?\",          â† Whisper STT\n",
    "  \"answer_text\": \"à¦¬à¦¨à§à¦§à§ à¦¹à¦²à§‹...\",         â† Phi-3.5 RAG\n",
    "  \"answer_audio\": \"base64...\",           â† Edge-TTS\n",
    "  \"language\": \"bengali\",\n",
    "  \"timings\": {\"stt\": 2.1, \"llm\": 7.5, \"tts\": 1.2, \"total\": 10.8}\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": ["## âœ… Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('âœ… Voice Pipeline â€” Complete!')\n",
    "print('=' * 60)\n",
    "print('''\n",
    "ğŸ¤ STT  : Whisper Medium â€” Bengali + English\n",
    "ğŸ§  LLM  : Phi-3.5 Mini 4-bit â€” NCTB RAG\n",
    "ğŸ”Š TTS  : Edge-TTS â€” bn-BD-NabanitaNeural\n",
    "âš¡ Speed : ~10-12s total per response\n",
    "\n",
    "Expected latency breakdown:\n",
    "  Whisper STT  : ~2-3s\n",
    "  Phi-3.5 RAG  : ~7-8s  \n",
    "  Edge-TTS     : ~1-2s\n",
    "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Total        : ~10-13s\n",
    "\n",
    "ğŸš€ à¦ªà¦°à¦¬à¦°à§à¦¤à§€ step:\n",
    "  â†’ Django REST API à¦¤à§‡ integrate à¦•à¦°à¦¾\n",
    "  â†’ Flutter app à¦¥à§‡à¦•à§‡ audio send/receive\n",
    "  â†’ Frustration detection add à¦•à¦°à¦¾\n",
    "  â†’ Quiz mode integrate à¦•à¦°à¦¾\n",
    "''')\n",
    "print('=' * 60)"
   ]
  }
 ]
}
